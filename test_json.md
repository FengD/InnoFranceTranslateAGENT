{
  "language": "fr",
  "segments": [
    {
      "start": 0.03,
      "end": 15.93,
      "text": "Ça y est maintenant on a trouvé le secret de l'intelligence! D'ici 10 ans on aura des machines aussi intelligentes que les humains! Yann Lequin, ou de l'autre côté de l'Atlantique, c'est plutôt Yann Lecun. Découvrir le mystère de l'intelligence et construire des machines intelligentes, je pense que c'est la seule manière de valider si des idées abstraites fonctionnent.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 16.06,
      "end": 23.72,
      "text": "En fait, le LLM, c'est plié. Maintenant, c'est parti dans une équipe produit. C'est plus de la recherche. Un peu, oui. J'étais pas prêt à cette... Elon M.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 20.33,
      "end": 27.5,
      "text": "Un peu, oui. Oui, il avait été mercredi, mais ça n'a pas marché.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 27.54,
      "end": 29.9,
      "text": "Ça me fait penser à Apple et son casque là.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 30.32,
      "end": 35.55,
      "text": "Apple essaie de rattraper leur retard et ils sortent",
      "speaker": "SPEAKER0"
    },
    {
      "start": 32.04,
      "end": 62.59,
      "text": "Apple essaie de... et il sorte un truc à bord qui est moins bon et qui est 7 fois plus cher. Donc non, je ne suis pas impressionné. Bientôt, ça sera dans les lunettes intelligentes et on aura les sous-titres qui s'affichent si on parle à quelqu'un dans une langue étrangère. Dès qu'une prédiction est violée, on est obligé d'y prêter attention parce que ça veut dire que notre modèle du monde était faux. C'est des réseaux convolutifs, c'est un peu mon invention qui date de 35 ans et qui détectent les objets qu'on a besoin de détecter. Je ne crois pas à l'application du calcul quantique à l'IA et en fait, je suis très sceptique sur le calcul quantique.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 56.88,
      "end": 56.9,
      "text": "Sous-titrage Société Radio-Canada",
      "speaker": "SPEAKER0"
    },
    {
      "start": 56.9,
      "end": 70.97,
      "text": "Je ne crois pas. Tu pourrais envisager qu'un LLM comme ça invente une langue et tu lui proposes une langue inventée peut-être par un autre LLM et qu'il le comprenne.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 70.97,
      "end": 81.98,
      "text": "Oui, absolument. Donc simplement en observant le monde pendant quelques mois ou même quelques centaines d'heures, on a plus d'informations, en fait, plus de données que la totalité du texte disponible sur Internet. Il y a des choses",
      "speaker": "SPEAKER1"
    },
    {
      "start": 81.98,
      "end": 82.75,
      "text": "qui te font peur",
      "speaker": "SPEAKER2"
    },
    {
      "start": 82.75,
      "end": 82.97,
      "text": "lire.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 82.97,
      "end": 83.11,
      "text": "Sous-titrage Société Radio-Canada",
      "speaker": "SPEAKER2"
    },
    {
      "start": 83.11,
      "end": 83.16,
      "text": "Merci.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 88.73,
      "end": 134.47,
      "text": "Ben nous y voilà, cet épisode tant attendu, moi je suis un... c'est marrant parce que tu n'as peut-être pas des groupies très souvent, mais je suis presque un groupie d'Yann Lequin, tu vois, autant j'ai des gens qui ont l'habitude d'avoir des groupies ici, tu vois, des Bob Sinclar, des artistes, des DJs, voilà, mais j'ai écouté beaucoup de tes épisodes, enfin les trois épisodes je crois, chez Lex Friedman. Chez Lex Friedman? Le dernier était dense, pointu. En plus, je ne connais pas Lex Friedman personnellement, mais il est très sharp. Donc, vous allez assez loin. Et moi, ce que je te propose aujourd'hui, Yann, c'est d'aller... Au moins de reposer les bases. Si on peut aller loin, je suis très content. Déjà aussi de reposer les bases de...",
      "speaker": "SPEAKER2"
    },
    {
      "start": 130.86,
      "end": 130.9,
      "text": "Merci.",
      "speaker": "SPEAKER0"
    },
    {
      "start": 130.9,
      "end": 131.27,
      "text": "D'accord.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 134.76,
      "end": 189.22,
      "text": "de l'IA, des LLM, aussi toi? ce que tu penses qui m'intéresse, c'est que j'ai eu sur Génération du Turself et fait des épisodes très sympas autour de l'IA, mais il y en a qui sont plus flamboyants ou qui vont faire fantasmer aussi. J'ai reçu des gens comme Laurent Alexandre, que tu connais peut-être de près, de loin, avec qui je m'éclate et que j'aime beaucoup d'ailleurs, avec qui on s'écharpe aussi dans chaque épisode. Mais voilà, et j'aimerais comprendre un peu pour toi de l'intérieur de chez Fer notamment où est-ce que vous en êtes qu'est-ce que tu vois qu'est-ce que tu comprends qu'est-ce que tu vois à travers tes Ray-Ban méta qui me filment en ce moment non qui ne me filment pas justement et puis j'aimerais aussi comprendre et peut-être pour pour",
      "speaker": "SPEAKER2"
    },
    {
      "start": 177.66,
      "end": 178.11,
      "text": "D'accord.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 189.72,
      "end": 231.29,
      "text": "les plus jeunes qui nous écoutent, comment on peut, en partant de France, finir un job qui est quand même l'un des plus hauts de l'IA dans le monde. Tu vois, il y a Sam Altman et Yann Lequin. Donc, peut-être se dire que c'est possible et comprendre comment on y arrive. En tout cas, comment tu y es arrivé, cette trajectoire des années 80. Tu voyais déjà, semble-t-il, pas mal de choses. Tu commençais à tripatouiller pas mal de choses. Voilà, cette fois comme programme. Ça va très bien. Je te propose avant tout ça, si tu veux bien, de te présenter, Yann. D'accord.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 231.29,
      "end": 262.97,
      "text": "euh Donc, Yann Lequin, ou de l'autre côté de l'Atlantique, c'est plutôt Yann Lecun. Et puis, ça ne s'écrit pas pareil parce que mon nom s'écrit en deux mots, mais aux Etats-Unis, les gens ne comprennent pas que le, ce n'est pas mon middle name. Donc, j'ai accolé les deux. Donc, je suis Chief AI Scientist, donc scientifique en chef à Meta. J'y suis depuis un peu plus de dix ans.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 250.02,
      "end": 250.41,
      "text": "– Hum.",
      "speaker": "SPEAKER0"
    },
    {
      "start": 262.97,
      "end": 290.11,
      "text": "York University, qu'on appelle NYU, donc une des grandes universités privées de New York depuis une vingtaine d'années. Et puis j'avais commencé avant ça ma carrière dans l'industrie, dans les laboratoires de recherche de la compagnie AT&T, une grosse compagnie de téléphone, c'est vrai qu'il y avait un labo mythique qui s'appelle Bell Labs, Bell Laboratories, dans lequel une grande partie de la technologie du monde moderne a été inventée, en particulier les transistors.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 284.46,
      "end": 304.47,
      "text": "– Très bien. Voilà. Intéressant. Je ne savais pas, tu vois, que NYU était privé, en l'occurrence avec un nom comme NYU. Tu aurais pu te dire que c'était public. Alors, les universités publiques, Princeton ou...",
      "speaker": "SPEAKER2"
    },
    {
      "start": 304.47,
      "end": 323.17,
      "text": "Princeton est privé, Columbia qui est aussi à New York est aussi privé. Il y a une université qui s'appelle City University of New York, CUNY, qui est publique, qui appartient à la ville. Et puis il y a d'autres universités d'État qui appartiennent à l'État de New York, l'État du New Jersey. Radcliffe University est publique dans le New Jersey.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 313.05,
      "end": 363.97,
      "text": "Ok, ouais, ouais. Ok, intéressant. Écoute, je trouve qu'aujourd'hui, en 2024, quand on échange, le monde a... a pris un 33 tonnes dans la gueule il y a 18 mois, en gros, avec la sortie de GPT-3. J'ai l'impression que toi, tu n'as même pas pris une piche nette et que ça ne t'a pas complètement bouleversé. Mais c'était un peu la vraie découverte pour l'humanité quand même, de la puissance potentielle d'un LLM. Et toi, j'ai l'impression qu'aujourd'hui, tu...",
      "speaker": "SPEAKER2"
    },
    {
      "start": 348.06,
      "end": 348.43,
      "text": "à la prochaine",
      "speaker": "SPEAKER0"
    },
    {
      "start": 364.43,
      "end": 376.66,
      "text": "Tu dis oui, c'est bien, mais c'est pas non plus... On est quand même très loin de tout ce qui est annoncé par beaucoup. Est-ce que tu peux m'expliquer un peu, me réexpliquer comment fonctionne un LLM et peut-être pourquoi c'est limité?",
      "speaker": "SPEAKER2"
    },
    {
      "start": 376.92,
      "end": 519.26,
      "text": "D'accord. Alors la première chose, c'est l'apparition de LLM, disons dans le public, qui a été révélée un petit peu avec ChatGPT à la fin 2022, n'a pas été vraiment du tout une révolution pour des gens qui sont dans la recherche de l'IA, parce que ce genre de technique existe depuis assez longtemps et puis on connaît leurs possibilités. Ce qui a choqué un peu tout le monde à la sortie de ChatGPT, c'est l'engouement du public pour ce genre de système. Il y a une histoire assez amusante, c'est que trois semaines avant l'apparition publique de ChatGPT, chez Meta, on avait sorti un LLM qui s'appelait Galactica et qui avait été entraîné de manière très spécifique pour aider les chercheurs à écrire des articles scientifiques. Donc ce système avait été entraîné sur la totalité de la littérature scientifique disponible publiquement. Et c'était fait pour aider les gens, surtout qu'il y en a plein dont l'anglais n'est pas la langue natale, donc c'est un peu compliqué d'écrire en anglais. Donc ça allait être très utile, on était très fiers de nous. Et quand on a rendu la démo disponible, elle a été assassinée, arrosée de vitrioles par Twitter. La sphère de Twitter en particulier, ou disons sur les réseaux sociaux, par des gens bien ou mal intentionnés, mais y compris des scientifiques eux-mêmes, disant ça va détruire le système de publication scientifique. Maintenant, n'importe quel imbécile peut écrire un article, qui apparaît être authentique, etc. Et donc, ça va être un gros danger, ça va détruire la société. Donc, au bout de trois jours, les chercheurs de fer qui avaient construit le système ont éteint la démo parce qu'ils se sont dit qu'ils n'en auraient pas la nuit. Et donc, le résultat de cette négativité a été fait qu'un outil qui aurait pu être très utile pour la communauté scientifique a disparu. Trois semaines plus tard, Chez GPT est sorti et là c'est la deuxième descente du Messie du paradis. C'était vraiment un choc pour nous en fait, le fait que les gens s'intéressent tant à ces technologies qui par ailleurs avaient été vraiment très fortement critiquées. Et Galactica n'était pas le premier système qu'on avait sorti comme ça, il y en a eu plusieurs les années précédentes.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 519.26,
      "end": 525.43,
      "text": "Il y avait eu sur Twitter aussi cet effet, chez Microsoft où il y avait eu un... Alors ça c'est beaucoup plus de hausse des tailles. Ouais.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 523.7,
      "end": 553.7,
      "text": "Beaucoup plus tôt, c'était Tai, un système qui avait été déployé en Chine pour conversation. Et quand ils l'ont déployé aux États-Unis et dans le reste, le truc a été trollé pendant 24 heures. Il est devenu raciste néo-nazi. Parce que les gens avaient compris que si on tapait des choses, le système utilisait ces phrases et les réutilisait dans les autres dialogues. Et donc, il s'est fait troller immédiatement.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 537.63,
      "end": 540.96,
      "text": "Il est devenu... néo-nazis.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 545.73,
      "end": 545.75,
      "text": "Merci.",
      "speaker": "SPEAKER0"
    },
    {
      "start": 545.75,
      "end": 546.31,
      "text": "Vous les utilisez?",
      "speaker": "SPEAKER2"
    },
    {
      "start": 546.31,
      "end": 546.33,
      "text": "Merci.",
      "speaker": "SPEAKER0"
    },
    {
      "start": 553.7,
      "end": 576.19,
      "text": "C'était une belle leçon pour Microsoft. Et puis, les différences culturelles entre la Chine et le reste du monde. Non, là, mais ce n'était pas un LLM à l'époque. Mais il y a eu pas mal de LLM. En fait, la révolution un petit peu technologique, ça a été le deep learning déjà, qui est avec nous depuis une quinzaine d'années. Ensuite,",
      "speaker": "SPEAKER1"
    },
    {
      "start": 564.01,
      "end": 576.23,
      "text": "Hum. Merci.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 576.23,
      "end": 576.24,
      "text": "Merci.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 576.24,
      "end": 582.34,
      "text": "Tu peux réexpliquer en une phrase ou deux à chaque fois le deep learning et le LLM si tu veux bien.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 581.63,
      "end": 611.63,
      "text": "D'accord. Alors, Deep Learning, c'est un ensemble de techniques pour entraîner les machines au lieu d'avoir les programmées directement. C'est-à-dire qu'on peut les entraîner à accomplir une tâche. Et très souvent, ça utilise ce qu'on appelle l'apprentissage supervisé. Donc, par exemple, on peut entraîner une machine à reconnaître des objets dans les images. On lui montre une image d'un chien, d'un chat, d'une table, d'une chaise. Donc, par exemple, disons une table. On attend la réponse du système. Si le système dit table, on ne fait rien.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 589.37,
      "end": 589.64,
      "text": "il y a",
      "speaker": "SPEAKER2"
    },
    {
      "start": 611.63,
      "end": 641.63,
      "text": "ou chaoucha. On lui dit non, la réponse c'est table. Et le système change ses paramètres internes, ses décoefficients en fait, dans des formules mathématiques très simples qui font des additions, des multiplications, de manière à ce que la sortie soit rapprochée de celle qu'on veut. Donc ça, ça marche très bien pour des choses comme la reconnaissance d'images, la traduction, la reconnaissance de paroles, des choses comme ça. Ça c'est le superviser. C'est le superviser. Alors il y a une autre méthode d'apprentissage dans laquelle les gens avaient mis beaucoup d'espoir,",
      "speaker": "SPEAKER1"
    },
    {
      "start": 623.43,
      "end": 623.8,
      "text": "Ok.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 641.63,
      "end": 681.33,
      "text": "il y a une dizaine d'années, sur lequel DeepMind, qui appartient à Google maintenant, s'était entièrement fondé, qui s'appelle l'apprentissage par renforcement. Et alors là, on ne dit pas à la machine quelle est la réponse correcte, on dit à la machine si la réponse qu'elle a produite est bonne ou pas bonne. Ou il donne un espèce de score. Et ça, c'est très pratique si on veut entraîner une machine, par exemple, à jouer aux échecs, au poker, au go, etc. Parce qu'à la fin, automatiquement, on peut déterminer à gagner la partie ou perdre. Donc, savoir si la manière dont elle a joué était bonne ou pas. Et en jouant des millions de parties, le système peut se raffiner et finalement être super humain.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 672.45,
      "end": 672.53,
      "text": "...",
      "speaker": "SPEAKER0"
    },
    {
      "start": 672.53,
      "end": 682.03,
      "text": "perdu. De loin?",
      "speaker": "SPEAKER2"
    },
    {
      "start": 681.46,
      "end": 681.68,
      "text": "de l'eau.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 683.06,
      "end": 694.12,
      "text": "J'allais dire de loin, on a l'impression qu'il y en a une où on lui apprend à réfléchir, mais ça ne doit pas être exactement le cas. Et dans l'autre, on lui dit vrai, faux, mais tout bête. Mais ce n'est pas le cas.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 694.18,
      "end": 706.99,
      "text": "Il y en a un où on lui donne la réponse et l'autre on lui dit simplement si c'est vrai ou faux. S'il y a beaucoup de réponses possibles, en fait c'est très très inefficace parce que le système doit essayer. Alors c'est ça la bonne réponse? Ou alors c'est ça? Ou alors c'est ça? Et à chaque fois...",
      "speaker": "SPEAKER1"
    },
    {
      "start": 700.04,
      "end": 709.02,
      "text": "C'est une bonne idée. C'est une chaise, c'est une pomme, c'est un avion.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 708.27,
      "end": 754.58,
      "text": "Voilà. Donc s'il y a 10 000 possibilités, ça risque de prendre du temps. Et dans le cas d'un jeu d'échecs, etc., c'est pas grave, on peut faire en sorte que le système joue des millions et des millions de parties. Donc ça finit par marcher. Mais il y avait beaucoup d'espoir que ce type d'apprentissage, qui se rapproche un petit peu de l'espèce de conditionnement à la Pavlov, serait la base de l'apprentissage chez les humains et les animaux. Il s'avère que non, pas du tout, c'est tellement inefficace qu'il n'y a absolument aucune chance qu'on puisse utiliser. de méthodes pour entraîner des systèmes, par exemple à conduire une voiture ou quoi que ce soit. Donc cet espoir a complètement disparu, DeepMind s'est complètement éorienté, a complètement abandonné en fait plus ou moins cette approche.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 749.04,
      "end": 754.6,
      "text": "Ok. Merci.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 754.6,
      "end": 754.61,
      "text": "Merci.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 754.61,
      "end": 758.31,
      "text": "Donc on a été supervisé. Le deuxième, tu m'as dit, c'est renforcement?",
      "speaker": "SPEAKER2"
    },
    {
      "start": 754.63,
      "end": 788.43,
      "text": "Donc on a dit... renforcement renforcement Et la troisième, c'est auto-superviser ou non-superviser. Et alors là, c'est ce qu'utilisent les LLM. C'est donc une méthode dont je me suis fait un petit peu le défenseur ou l'avocat depuis près d'une dizaine d'années. Et qui est une technique dans laquelle on n'entraîne pas le système à faire une tâche particulière, mais on l'entraîne à représenter, comprendre la structure de l'entrée. Alors, dans le cas du texte, dans le cas des LLM, par exemple,",
      "speaker": "SPEAKER1"
    },
    {
      "start": 761.23,
      "end": 761.55,
      "text": "OK.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 788.43,
      "end": 893.87,
      "text": "On prend un bout de texte, une séquence de quelques centaines ou quelques milliers de mots, et on le corrompt. On masque certains mots, on les remplace par des marqueurs blancs, ou on substitue certains mots, etc. Donc on rend le... Le texte ou l'entrée, ça peut marcher avec une image aussi, corrompu. Et ensuite, on entraîne un grand réseau de neurones, donc un système de deep learning, à prédire les mots qui manquent ou les mots qui étaient faux. C'est-à-dire à reconstruire l'entrée complète à partir d'une entrée incomplète. Donc c'est basé sur une vieille idée qui s'appelle les auto-encodeurs de débrutage. Ça date des années 80, c'est vraiment vieux. Mais l'application à la compréhension de texte, en fait, Ça date des années 2016 ou 2015. Et en fait, ce faisant, en apprenant à trouver les mots qui manquent, ce système élabore une représentation du langage, du texte, qui contient la signification, la grammaire, la syntaxe, enfin tout, l'orthographe. Et donc ensuite, on peut entraîner le système à prédire des mots qui manquent. Alors il ne fait pas une prédiction exacte, parce qu'on ne peut pas, si je dis, le chat chasse le blanc dans la cuisine, Blanc, ça peut être une souris, ça peut être un jouet, ça peut être le spot d'un laser ou une mouche. Donc on ne peut pas exactement prédire le mot qui manque, mais ce qu'on produit, c'est une espèce de score pour tous les mots possibles dans le dictionnaire. Ou une probabilité, quelque chose comme ça.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 874.71,
      "end": 875,
      "text": "Hum.",
      "speaker": "SPEAKER0"
    },
    {
      "start": 890.47,
      "end": 897.7,
      "text": "Ok. A priori, il ne sera pas la chaise ni la table. Voilà.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 896.58,
      "end": 914.89,
      "text": "de la chasse que vous avez. donc il n'y a que certains mots qui sont possibles à certains endroits etc alors ensuite en fonction de l'architecture qu'on donne à ce réseau alors déjà ce qui est intéressant c'est qu'on peut l'entraîner avec des textes de plusieurs langues et le système en fait apprend à représenter le sens du texte indépendamment de la langue ce qui est assez extraordinaire ça c'est fou mais c'est fou",
      "speaker": "SPEAKER1"
    },
    {
      "start": 908.75,
      "end": 938.57,
      "text": "Hum. Ça c'est fou. Mais tu vois, typiquement, ça c'est un des trucs où tu te dis mais... Avant le lancement de ChatGPT, personne, j'imagine que vous l'aviez tous ça le changement. Oui mais en fait il n'y avait rien d'aussi efficace que, enfin tu vois, juste la traduction, si je prends que ça et que j'isole ce truc, Google Trad ou tous ces trucs, c'était pas aussi bien que ce que fait OpenAI. Non si.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 921.83,
      "end": 968.2,
      "text": "Mais Non, si, si, en fait, c'était mieux. Les systèmes de traduction spécialisés sont préentraînés de manière autosupervisée de cette manière-là, mais ils sont plus efficaces pour la traduction que des systèmes peu génériques comme ChatGPT. Mais ce qu'on constate, c'est que plus on entraîne des systèmes avec beaucoup de données, plus ils deviennent bons pour tout un tas de tâches, même des tâches pour lesquelles ils ne sont pas spécialisés. Donc, il y a un peu cet effet d'émergence. Mais par exemple, chez Meta, on a un système qui s'appelle Seamless, qui peut traduire 200 langues dans n'importe quel directeur.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 939.97,
      "end": 940.31,
      "text": "Si tu as une autre question...",
      "speaker": "SPEAKER2"
    },
    {
      "start": 968.2,
      "end": 989.6,
      "text": "et qui peut traduire en temps réel, c'est-à-dire avec un délai de deux secondes, qui peut faire texte à texte, texte à voix, voix à texte et voix à voix, y compris pour des langues qui ne sont pas écrites, ce qui est hallucinant. Et donc ça, ce n'est pas encore dans les mains de tous les utilisateurs, mais bientôt, ça sera dans les lunettes intelligentes et on aura les sous-titres qui s'affichent Si on parle à quelqu'un dans une langue étrangère.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 990.04,
      "end": 1010.49,
      "text": "Est-ce que tu peux J'imagine que non. Du coup, je te pose la question avant de te la poser. Tu pourrais envisager que... un LLM comme ça invente une langue Euh... et tu lui proposes une langue inventée peut-être par un autre LLM et qu'il la comprenne.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 1010.76,
      "end": 1090.85,
      "text": "Oui, absolument, c'est possible. En fait, ce genre d'expérience a déjà été fait, où on met deux agents en relation l'un avec l'autre et puis on essaie de les entraîner à résoudre un problème et on construit la tâche de manière à ce que individuellement, les deux agents ne peuvent pas résoudre la tâche, ils sont obligés de collaborer. On les entraîne au départ à utiliser l'onglet pour communiquer et en fait, ils finissent par inventer une espèce de langue efficace pour communiquer entre eux. C'est amusant comme expérience. Mais en fait, pour l'apprentissage autosupervisé, idée que on fait une corruption d'une entrée, puis on entraîne le système à régénérer l'entrée complète. Alors, régénérer, ça veut dire prédire, en fait, l'entrée. Et c'est pour ça qu'on parle de modèle génératif, parce qu'on génère l'entrée, on régénère l'entrée. Et en fonction de l'architecture des systèmes qu'on entraîne pour faire ça, on peut ensuite leur donner un texte et leur demander de prédire le mot suivant dans le texte. Et ensuite, ce qu'on peut faire, c'est prendre le mot que le système a prédit. Alors, il produit une distribution probabilité, mais on prend un des mots qui a une grande probabilité et on le met dans l'entrée. C'est-à-dire qu'on décale tous les mots de l'entrée et on lui rajoute le mot qu'il a lui-même prédit. Et ensuite, on lui demande de prédire le deuxième mot.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 1085.5,
      "end": 1086,
      "text": "Ok.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 1092.08,
      "end": 1109.41,
      "text": "On fait décaler ça dans l'entrée, le troisième mot, etc. Ça s'appelle la prédiction autorégressive. C'est un vieux concept, mais qui date des années 40-50. C'est pas récent, mais avec ça, on peut faire produire des textes relativement longs, en fait, assez LLM.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 1096.67,
      "end": 1115.87,
      "text": "Ok. Ce qui explique que tu peux leur demander d'écrire des histoires ou des choses comme ça en disant...",
      "speaker": "SPEAKER2"
    },
    {
      "start": 1115.7,
      "end": 1117.48,
      "text": "... répondre à des questions?",
      "speaker": "SPEAKER1"
    },
    {
      "start": 1117.22,
      "end": 1120.29,
      "text": "un coin sur la banquise, etc. Et puis voilà, vous répondez à des questions.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 1120.31,
      "end": 1158.6,
      "text": "ou de traduire un texte en lui demandant, traduis ce texte de français à anglais, etc. On fait tout un tas d'instructions. Mais la limitation de ça, c'est que ces systèmes ne sont entraînés que sur du texte, d'une part, et d'autre part, ne peuvent pas vraiment réfléchir. C'est-à-dire, si on leur pose une question simple, et qu'on leur dit, répond par oui ou non, c'est-à-dire, est-ce que 2 plus 2 égale 4, répond par oui ou non, ils vont répondre oui ou non, et les ressources qu'ils vont dédier à ça, sinon dans un réseau de neurones qui a 46 couches ou 92 couches, enfin peu importe, et produit le token, le mot oui ou non.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 1158.6,
      "end": 1162.3,
      "text": "Avec une possibilité qu'il y ait une erreur en plus. Dans le Wii U, non?",
      "speaker": "SPEAKER2"
    },
    {
      "start": 1158.62,
      "end": 1175.12,
      "text": "D'accord. Alors c'est possible qu'il y ait une erreur, mais enfin bon, 2 plus 2 égale 4, est-ce que c'est vrai? Bon, c'est facile à répondre. Maintenant, si on pose une question beaucoup plus compliquée, c'est-à-dire par exemple l'arithmétique avec des nombres assez importants, ou quelle est la racine carrée de machin, ou...",
      "speaker": "SPEAKER1"
    },
    {
      "start": 1166.4,
      "end": 1166.67,
      "text": "Ok.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 1175.27,
      "end": 1212.6,
      "text": "une question complètement insoluble. Est-ce que, par exemple... tout nombre pair et la somme de deux nombres premiers qui s'appelle la conjecture de Goldbar en mathématiques qui n'est pas prouvée c'est une hypothèse, une conjecture en fait qui a l'air d'être vraie mais qui n'est pas prouvée il n'y a pas de preuve et on dit répond par oui ou non le système ne peut pas réfléchir à ça il va toujours et idd, la même quantité de calcul que répondre à 2 plus 2 égale 4. Donc il n'y a pas de possibilité vraiment de raisonnement dans ces systèmes-là, qui leur permet de réfléchir un peu plus à des questions compliquées et un peu moins à des questions moins compliquées.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 1203.34,
      "end": 1203.39,
      "text": "Merci.",
      "speaker": "SPEAKER0"
    },
    {
      "start": 1203.39,
      "end": 1203.49,
      "text": "Merci.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 1203.49,
      "end": 1203.54,
      "text": "Merci.",
      "speaker": "SPEAKER0"
    },
    {
      "start": 1212.8,
      "end": 1218.03,
      "text": "Donc, c'est là où on pourrait enlever le mot intelligence. C'est une intelligence qui est factible.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 1218.03,
      "end": 1278.03,
      "text": "Alors elle n'est pas factice parce que ces systèmes sont entraînés avec une quantité de données tellement énorme qu'en fait ils peuvent régurgiter des solutions qu'ils ont déjà apprises. Donc 2 plus 2 égale 4 c'est dans leur mémoire, il n'y a pas de problème. Pour des chiffres, des nombres à plusieurs chiffres, c'est pas dans la mémoire. Donc là ils sont obligés de faire appel à une calculatrice et maintenant il y a des systèmes de ce type-là qui savent le faire. Ils savent que quand on leur pose un problème arithmétique, il faut qu'ils appellent une calculatrice. Plutôt que d'aller chercher dans les... Plutôt que de régurgiter ça dans leur mémoire. Mais il n'y a pas vraiment de capacité de raisonnement, d'invention de choses nouvelles, c'est plutôt de la régurgitation. Donc c'est un petit peu comme, vous savez, dans une classe, quand il y a une classe de maths, il y a deux sortes d'écoliers ou d'étudiants. Il y a ceux qui comprennent vraiment ce qui se passe derrière les maths, et puis ceux qui apprennent par cœur. Et puis on leur enseigne les éditions, multiplications, soustractions, etc. Et puis on leur pose un problème, et puis on vient de leur enseigner la multiplication.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 1267.63,
      "end": 1268.07,
      "text": "et puis ceux",
      "speaker": "SPEAKER0"
    },
    {
      "start": 1278.03,
      "end": 1325.65,
      "text": "la bonne chose à faire parce que c'est ça qu'ils viennent d'apprendre. Et donc, les LLM, aujourd'hui, sont un peu comme ça. Ils apprennent par cœur un peu. Ils savent, bien sûr, adapter une réponse, mais pas toujours. Donc, si on leur pose un problème un petit peu classique, un puzzle du genre, il y a le problème de la chèvre, le chou et le loup qu'on doit transporter d'un côté et de l'autre de la rivière avec un bateau qui ne peut transporter que deux choses à la fois. Et comment faire en sorte, quelle est la séquence pour faire en sorte que, le loup ne mange pas la chèvre ne mange pas le chou etc donc ne peut pas être en même temps tout seul on pose ce problème là à Tchadjépté il répond immédiatement sans problème mais bien sûr il rigurgite ça de sa mémoire parce que c'est l'exemple là",
      "speaker": "SPEAKER1"
    },
    {
      "start": 1310.91,
      "end": 1312.3,
      "text": "Ne mange pas la chèvre.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 1312.3,
      "end": 1314.98,
      "text": "Le chef ne mange pas le chou, etc.",
      "speaker": "SPEAKER0"
    },
    {
      "start": 1323.89,
      "end": 1326.47,
      "text": "Parce que cet exemple-là...",
      "speaker": "SPEAKER2"
    },
    {
      "start": 1326.47,
      "end": 1336.94,
      "text": "Par contre, on change un petit peu le problème pour que la solution soit différente et il rigore toujours la même réponse. Donc ça veut dire qu'il ne réfléchit pas vraiment.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 1337.05,
      "end": 1351.16,
      "text": "Et si je change la chèvre par une antilope et puis que je le loue par un tigre ou un lion et puis le chou par je sais pas quoi, est-ce qu'il est capable de le comprendre?",
      "speaker": "SPEAKER2"
    },
    {
      "start": 1351.16,
      "end": 1363.63,
      "text": "est capable de comprendre parce que la représentation de chaque objet, en fait, la représentation d'une antilope et d'une chèvre va être similaire à l'intérieur du réseau de neurones, etc.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 1359.16,
      "end": 1367.36,
      "text": "dans le... – OK. Il va faire une sorte de traduction finalement de la chèvre vers l'antilope?",
      "speaker": "SPEAKER2"
    },
    {
      "start": 1367.34,
      "end": 1392.86,
      "text": "Alors, on peut peut-être voir ça comme ça, mais c'est plutôt qu'en fait, une entité, enfin quelque chose, un objet, un mot, est représenté par une séquence de nombres. On appelle ça un vecteur. Et les séquences de nombres qui représentent des entités similaires, en fait, sont similaires. Donc, peuvent être substituées l'une par l'autre. Et donc, un prédateur par un prédateur, etc. Donc, je pense que ce genre de choses marchent.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 1372.39,
      "end": 1404.45,
      "text": "Oui. c'est ça Il va comprendre que le chou ne mange personne, a priori, mais qu'il risque d'être mangé par plutôt la chèvre ou l'antilope. Et alors, ce qu'on dit là, c'est que...",
      "speaker": "SPEAKER2"
    },
    {
      "start": 1394.7,
      "end": 1399.04,
      "text": "mais qui risque d'être... Il est vraiment",
      "speaker": "SPEAKER1"
    },
    {
      "start": 1404.98,
      "end": 1412.5,
      "text": "Je reviens sur la question d'intelligence, on est plus mémoire versus intelligence quand même dans le LLM finalement.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 1412.5,
      "end": 1414.7,
      "text": "C'est plus régrégitation que raisonnement.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 1414.73,
      "end": 1444.65,
      "text": "Ok. Et pourtant, ça donne de temps en temps un truc... une sensation d'intelligence qui est assez frappante. Mais de ce que je comprends aussi, vous êtes capable, chez Meta et potentiellement ailleurs, de la rendre un peu factice, notamment potentiellement en orientant vers, là tu disais, une sorte de calculatrice ou d'autres choses selon les questions, de comprendre que ça, ce n'est pas dans la mémoire, mais c'est plus ailleurs qu'il va falloir aller chercher la réponse, c'est ça?",
      "speaker": "SPEAKER2"
    },
    {
      "start": 1443.42,
      "end": 1473.42,
      "text": "chercher la réponse, c'est ça? Il y a beaucoup de gens qui travaillent en ce moment sur essayer d'améliorer les performances des LLM en les augmentant avec des outils, soit évidemment des calculatrices, des systèmes de résolution d'équations, des choses comme ça, qui seraient appelés à bon escient, ou une technique qu'on appelle RAG, donc en anglais ça veut dire Retrieval Augmented Generation, et ça veut dire en fait interroger une base de connaissances, un moteur de recherche,",
      "speaker": "SPEAKER1"
    },
    {
      "start": 1462.5,
      "end": 1462.84,
      "text": "ou",
      "speaker": "SPEAKER0"
    },
    {
      "start": 1473.42,
      "end": 1525.95,
      "text": "Pour une question, si on pose la question à LLM, quel était le BIP de la France en 2015? C'est très probable qu'il n'a pas ça dans sa mémoire. Mais par contre, il pourrait interroger un moteur de recherche ou aller chercher la réponse sur Wikipédia. Donc des systèmes augmentés, des LLM augmentés, peuvent interroger un moteur de recherche. insérer la réponse dans le prompt, c'est-à-dire la rajouter à la question qu'a tapé l'utilisateur, et ensuite transformer ça en une réponse en texte lisible. Donc les agents, les assistants virtuels de Meta, Meta AI, font ça, en utilisant des moteurs de recherche, des sources d'informations, etc. Et ça, c'est très utile. Mais quand même, ça n'élève pas complètement les limitations de ces systèmes-là au niveau raisonnement,",
      "speaker": "SPEAKER1"
    },
    {
      "start": 1498.67,
      "end": 1498.87,
      "text": "– Sous-titrage Société Radio-Canada",
      "speaker": "SPEAKER0"
    },
    {
      "start": 1525.95,
      "end": 1546.37,
      "text": "et autres. On a un peu le même problème pour les systèmes qui produisent du code automatiquement. Dans la mesure où c'est du code un petit peu stéréotypé, on peut utiliser une espèce de patron qu'on peut adapter. Le système marche bien, mais pour faire le design d'un système logiciel un peu nouveau, il n'y a plus personne.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 1546.86,
      "end": 1604,
      "text": "Oui, et en fait, j'imagine que dès que tu es assez spécialiste dans un truc, tu vois souvent quand même assez vite la limite. Ce que je veux dire, c'est que moi, ce qui me scotche à chaque fois, c'est quand je lui dis est-ce que tu peux, je ne sais pas, je vais lui dire, expliquer telle problématique à mes enfants de manière très simple ou à la manière de Victor Hugo ou à la manière, c'est toujours, waouh, tu prends vraiment une claque. Cependant, quand j'écris, je ne sais pas, mes newsletters le dimanche, J'ai un raisonnement, j'ai une idée, j'ai envie de quelque chose. C'est pas dingue, quoi. Cela dit, ce que j'adore, c'est la manière de penser « qui est-ce qui m'a dit ça dans un épisode? » Mais de se dire que c'est la quatrième personne ou la cinquième personne dans un brainstorm, tu vois. Un peu, oui. Et quand tu demandes des idées, t'as toujours un ou deux trucs qui peuvent vraiment changer la donne. Absolument, oui. Ça, c'est dans de la mémoire, en fait. C'est ça.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 1602.45,
      "end": 1634,
      "text": "Ça, c'est dans le... Et c'est ça en fait que les gens, dans les techniques d'IA générative, que ce soit pour le texte, l'image, la vidéo, le son, etc. C'est une bonne manière d'avoir un interlocuteur avec qui on peut écouter, qui peut suggérer des idées ou inspirer en fait. Donc ça c'est très utile. Effectivement, on est un petit peu hypnotisé par le fait que ces systèmes manipulent la langue de manière très claire.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 1622.04,
      "end": 1622.39,
      "text": "Hum.",
      "speaker": "SPEAKER0"
    },
    {
      "start": 1634,
      "end": 1664,
      "text": "de l'intelligence, mais en fait c'est faux. C'est-à-dire que finalement, manipuler la langue, c'est simple. Et la raison pour laquelle c'est simple, c'est que la langue est composée d'entités discrètes des mots. Il n'y a qu'un nombre fini de mots dans le dictionnaire. En plus, dans les LLM, on réduit ça à ce qu'on appelle des tokens, qui sont des sous-mots, en fait. Il y en a typiquement, dans un LLM, 30 000 possibles ou quelque chose comme ça, 100 000 possibles. C'est utile pour des langues comme l'allemand, où on peut construire des mots",
      "speaker": "SPEAKER1"
    },
    {
      "start": 1636.11,
      "end": 1636.35,
      "text": "C'est-à-dire que...",
      "speaker": "SPEAKER0"
    },
    {
      "start": 1664,
      "end": 1669.96,
      "text": "en accolant des mots les uns aux autres donc il faut pouvoir les décomposer",
      "speaker": "SPEAKER1"
    },
    {
      "start": 1666.57,
      "end": 1675.11,
      "text": "donc il faut... J'imagine tous les synonymes en français aussi que tu as en fait.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 1674,
      "end": 1699.34,
      "text": "Oui, mais ça, le système fait ça automatiquement parce qu'il sait que les synonymes apparaissent dans des contextes similaires. Donc, il va automatiquement pouvoir faire ça. Mais on est hypnotisé par le fait que, à cause du fait que le système manipule la langue et que les seules entités avec lesquelles on est habitué qui peuvent manipuler la langue sont d'autres humains intelligents, on a l'impression qu'ils sont intelligents. Mais il ne faut pas se leurrer, c'est faux.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 1676.04,
      "end": 1708.59,
      "text": "– OK. Donc en fait tu dis que toutes ces personnes qui paraissaient très intelligentes en utilisant des grands mots étaient en fait potentiellement des idiots?",
      "speaker": "SPEAKER2"
    },
    {
      "start": 1701.77,
      "end": 1739.18,
      "text": "Sous-titrage Société Radio-Canada Il y a ça aussi. C'est pas exclu. Des gens qui sont habitués à faire des arguments plutôt grandiloquents en utilisant la rhétorique et des mots savants plutôt qu'en discutant sur la substance. C'est peut-être une division, c'est un peu un cliché, mais entre les scientifiques des sciences dures qui essaient de ne pas trop s'embarrasser de rhétorique et plutôt discuter de la substance.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 1710.21,
      "end": 1710.57,
      "text": "Merci.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 1710.57,
      "end": 1710.68,
      "text": "C'est pas possible.",
      "speaker": "SPEAKER0"
    },
    {
      "start": 1710.68,
      "end": 1710.92,
      "text": "C'est pas possible.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 1710.92,
      "end": 1711.12,
      "text": "explique",
      "speaker": "SPEAKER0"
    },
    {
      "start": 1711.12,
      "end": 1711.19,
      "text": "Sous-titrage Société Radio-Canada",
      "speaker": "SPEAKER2"
    },
    {
      "start": 1711.19,
      "end": 1711.22,
      "text": "Merci.",
      "speaker": "SPEAKER0"
    },
    {
      "start": 1711.22,
      "end": 1712,
      "text": "Ce n'est pas exclu.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 1712,
      "end": 1737.41,
      "text": "ou des Merci. Merci.",
      "speaker": "SPEAKER0"
    },
    {
      "start": 1739.18,
      "end": 1743.2,
      "text": "et puis les gens dont le métier est d'argumenter.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 1744.16,
      "end": 1770.78,
      "text": "Donc si je lis, merci pour ces explications Yann déjà, c'est très clair. Et je n'avais pas exactement tout, tout, tout. J'avais compris un certain nombre de choses là-dedans, mais pas tout, notamment le régressif, auto-régressif et puis aussi le supervisé, non-supervisé ou auto-supervisé, ça c'est aussi intéressant.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 1761.07,
      "end": 1763.15,
      "text": "Autorégressif. Autorégressif et prédégressif.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 1772.11,
      "end": 1818.72,
      "text": "Quand je t'écoute, donc je comprends ce que tu peux dire par moments ailleurs, c'est que... J'aimerais bien t'entendre le redire par tes mots, mais... Tu prends deux exemples sur un enfant de zéro à six mois ou un exemple sur le permis de conduire qui sont, je trouve, assez passionnants. C'est de dire qu'en fait, il manque des choses pour réussir dans la structure même, l'architecture même du LLM. On ne peut pas faire plein de choses. Donc, on est encore très loin, en tout cas, au moins là-dessus, de l'intelligence artificielle générale qui est prédite par beaucoup pour demain, dans trois ans, on ne sait pas. cas là, c'est pas du tout la bonne voie. Ça veut pas dire que c'est une mauvaise voie, ça sert à d'autres choses. C'est pas la bonne voie.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 1817.32,
      "end": 1938.77,
      "text": "Merci. C'est pas le... C'est utile, mais c'est sur l'autoroute qui nous mène à l'intelligence de niveau humain. Je n'aime pas beaucoup la phrase intelligence générale parce que l'intelligence humaine est très spécialisée. Mais disons sur la voie qui va nous mener à ça, l'LLM, c'est un peu une bretelle de sortie. C'est-à-dire que c'est très utile, il faut développer ces technologies, ça sert à plein de choses. Mais ce n'est pas le secret final, l'ingrédient qui nous manquait pour atteindre l'intelligence humaine. La raison de ça est multiple, mais on peut se poser la question, pourquoi on a des systèmes comme ça qui puissent produire du texte dans le style Victor Hugo, ou même passer l'examen du barreau? Mais pourquoi on n'a pas de voiture autonome qui se conduise toute seule, niveau 5, sans intervention humaine? Pourquoi on n'a pas de robot domestique qui puisse faire tous les travaux ménagers, débarrasser la table, remplir la vaisselle, etc.? Il s'avère que composer avec le monde physique, comprendre le monde physique est beaucoup, beaucoup, beaucoup plus compliqué que comprendre la langue. Alors ça, c'est très surprenant pour les humains parce qu'on a l'impression que la manipulation de la langue, c'est ça qui requiert l'intelligence de niveau humain. Mais en fait, non, la langue, c'est facile, c'est simple à modéliser avec les LLM. Comprendre le monde physique, c'est beaucoup plus compliqué. Et c'est pour ça qu'on peut se poser la question de savoir, enfin, le défi, la décennie qui vient dans la recherche en IA, c'est des systèmes qui puissent comprendre le monde physique, qui aient une mémoire persistante, c'est-à-dire un peu comme les humains, on a un truc spécial dans le cerveau qui s'appelle l'hippocampe, qui sert la mémoire à court terme et à long terme, mémoire factuelle, etc., mémoire épisodique, mémoire de travail. Si on n'a pas d'hippocampe, on ne peut pas se rappeler de choses pendant plus de à peu près 90 secondes. Il y a des gens comme ça qui ont eu des accidents vasculaires cérébraux,",
      "speaker": "SPEAKER1"
    },
    {
      "start": 1934.26,
      "end": 1934.67,
      "text": "Hum.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 1938.77,
      "end": 1997,
      "text": "des problèmes de hippocampe, ils ne se rappellent pas de choses d'une minute à l'autre ou d'une heure à l'autre. Et des systèmes qui sont capables de raisonner, donc de penser en passant du temps et l'énergie à un problème particulier et des systèmes capables de planifier. Alors on a l'impression que ces quatre capacités, compréhension du monde, mémoire persistante, raisonnement et planification, sont quand même des composants essentiels de l'intelligence et les LLM en sont essentiellement incapables. aujourd'hui. Donc on n'est pas du tout prêt à reproduire non seulement l'intelligence humaine, mais même l'intelligence des animaux. C'est-à-dire un chat de gouttière est très capable de comprendre le monde physique, a une mémoire persistante, peut certainement planifier et dans une certaine mesure raisonner aussi. Donc comment faire ça? Et pourtant ils ne savent pas lire. Ils ne savent pas lire, mais finalement, parce que ce n'est pas utile pour un chat de savoir lire. Donc l'évolution n'a pas construit cette capacité chez eux, ni de manipuler la langue, parce que ce n'était pas très utile pour eux.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 1986.62,
      "end": 1987.2,
      "text": "Pardonnez-moi de ne pas lire.",
      "speaker": "SPEAKER0"
    },
    {
      "start": 1987.2,
      "end": 1987.26,
      "text": "Merci.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 1989.27,
      "end": 1989.32,
      "text": "Merci.",
      "speaker": "SPEAKER0"
    },
    {
      "start": 1989.32,
      "end": 1989.34,
      "text": "Merci.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 1989.34,
      "end": 1991.21,
      "text": "C'est pas utile pour un chat de savoir lire.",
      "speaker": "SPEAKER0"
    },
    {
      "start": 1997.12,
      "end": 2009.57,
      "text": "Et tout ça n'est pas inné en plus, on peut le dire. Il y a peut-être certaines choses qui sont innées dans tout ça, mais c'est quelque chose qui l'apprend par le regard, l'audition, l'observation du monde physique.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 2004.34,
      "end": 2040.13,
      "text": "par le regard faiteux. Merci. l'observation. Voilà. Alors, on peut se dire, c'est un peu, comment dire, troublant parce que les systèmes, les LLM qu'on entraîne, on les entraîne avec essentiellement la totalité du texte disponible publiquement sur Internet. La quantité de données est typiquement de 10 000 milliards de mots, de tokens. Donc, c'est un 1 avec 13 0 derrière. Un token, c'est typiquement représenté par deux octets ou trois octets, quelque chose comme ça. Donc, ça fait une quantité d'informations qui est de l'ordre de un 2 avec 13 0 derrière.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 2032.14,
      "end": 2032.54,
      "text": "Hmm.",
      "speaker": "SPEAKER0"
    },
    {
      "start": 2040.13,
      "end": 2070.13,
      "text": "d'octets pour entraîner. Ça nous prendrait environ 100 000 ans à lire tout ça à raison de 12 heures par jour. Donc c'est incroyable comme quantité d'informations. On se dit si on entraîne un système avec ce genre de données, ça va être vraiment intelligent. Mais en fait non, parce que la connaissance qui est représentée par le texte est extrêmement parcellaire et très restreinte aux choses qui sont intéressantes pour les humains.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 2042.63,
      "end": 2042.97,
      "text": "pour entraîner",
      "speaker": "SPEAKER2"
    },
    {
      "start": 2070.13,
      "end": 2130.13,
      "text": "de connaissances de base sur le monde qui sont représentées par le texte. Et la manière dont on peut se convaincre de ça, c'est que les psychologues nous disent qu'un enfant de 4 ans, dans sa vie, a été éveillé un total d'environ 16 000 heures dans les 4 premières années de la vie. Et si on essaie de mettre un chiffre sur la quantité d'informations qui est arrivée au cerveau, par exemple au contexte visuel ou par le toucher, c'est de l'ordre de 10 à la puissance 15 octets. Donc c'est un 1 avec 15 zéros, c'est-à-dire 50 fois plus que la totalité du texte disponible publiquement avec lesquels on entraîne l'ALM. Donc simplement en observant le monde pendant quelques mois ou même quelques centaines d'heures, on a plus d'informations, plus de données que la totalité du texte disponible sur Internet. Ce que ça veut dire, c'est qu'on n'arriverait jamais à l'intelligence de niveau humain,",
      "speaker": "SPEAKER1"
    },
    {
      "start": 2106.79,
      "end": 2107.25,
      "text": "ça",
      "speaker": "SPEAKER0"
    },
    {
      "start": 2130.13,
      "end": 2144.05,
      "text": "en entraînant simplement des machines sur du texte. Il va falloir les entraîner sur la vidéo, sur de l'image, de manière auto-supervisée. Et c'est là que le bas blesse, on ne sait pas comment faire. Enfin, maintenant, on commence à avoir des idées, mais c'est beaucoup plus compliqué que le texte.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 2144.07,
      "end": 2189.06,
      "text": "Oui, je comprends ce que tu dis. Finalement, à un moment, on peut se dire... On aura des robots qui se baladeront, qui ressentiront les choses. On aura des lunettes qui vont enregistrer le monde pour comprendre des choses. Après, il reste le toucher, l'odorat, l'ouïe, des tas de choses, des sensations. Finalement, j'enregistrais avec quelqu'un qui s'appelle le professeur Gérard Saillant, je ne sais pas si tu vois, de l'Institut de la moelle et du cerveau. Il disait qu'il y a aussi une... Il n'excluait pas, c'est assez intéressant, qu'il y ait potentiellement une forme de télépathie. C'est quand même quelqu'un de très sérieux, mais il dit en fait, le cerveau émet des ondes et on ne sait pas. Il ne m'a pas dit que c'était le cas. Attention, je ne suis pas scientifique et vous l'êtes, lui et toi. Mais il dit, en fait, ce n'est pas exclu. Enfin, on ne sait pas exactement...",
      "speaker": "SPEAKER2"
    },
    {
      "start": 2180.5,
      "end": 2180.52,
      "text": "Merci.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 2189.58,
      "end": 2225.49,
      "text": "tout ce qu'on ressent et jusqu'où ça va et comment, c'est pas à poser. Mais la question que je te pose avant de rentrer justement sur peut-être GEPA et d'autres types de choses que vous travaillez, Est-ce qu'on ne pourrait pas envisager de tricher et de dire à la machine de lui rentrer textuellement tout ça? Lui dire en fait quand je touche, il se passe ça, c'est très compliqué. Mais quand je vois et dans les six premiers, se mettre à la place d'un bébé, dire est-ce que ça ne peut pas être une option de lui dire plutôt que de lui faire ressentir, ça va être compliqué, puis on ne va pas y arriver, je vais tricher, je vais lui envoyer comme sur les calculettes dont tu parlais tout à l'heure.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 2225.25,
      "end": 2309.43,
      "text": "C'est un peu ce que les gens font en ce moment. C'est-à-dire, il y a une phase de... Après le pré-entraînement sur les données publiques des LLM, on les ajuste, on les affine en leur posant des questions et puis ensuite en leur faisant produire plusieurs réponses et ensuite en engageant des personnes à donner un score à chacun des réponses possibles ou peut-être à proposer une meilleure réponse à la réponse en question ou simplement à donner faire en sorte que on puisse affiner la machine pour certains types de questions. Et si on engage suffisamment de milliers de personnes, qu'on dépense des centaines de millions là-dessus, ça coûte très cher de faire ça, mais ces opérations qui sont déployées par Meta, Google, Microsoft et donc OpenAI, on arrive finalement à couvrir une grande partie des questions que les gens peuvent poser et donc à peu près à couvrir un petit peu tous les... Mais c'est quand même insuffisant. Il y a toujours des questions qui vont sortir un petit peu des ornières et pour lesquelles le système ne va pas être entraîné et va répondre n'importe quoi. Parce qu'il n'a pas les capacités de raisonnement vraiment pour produire ses réponses lui-même.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 2290.9,
      "end": 2319.47,
      "text": "Quel usage? Comme je dis, j'ai utilisé le mot tricher, mais en fait, on ne raisonne pas. On triche, on lui donne des shortcuts, des raccourcis.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 2314.45,
      "end": 2383.98,
      "text": "Sous-titrage Société Radio-Canada Alors, il y a évidemment des systèmes aujourd'hui qui sont déjà déployés, dans lesquels on prend... Il y a deux options. Je vais écrire la première. La première consiste à ce qu'on appelle early fusion en anglais. Donc, ça consiste à prendre des images ou des vidéos, à les découper en petits blocs, petits carrés, et traiter ces carrés, en fait, comme des mots dans un texte. Et donc, quand on entraîne le système, non seulement la séquence de mots de la question, mais aussi la séquence de patchs d'images ou de vidéos ou d'audio tokenisés comme si c'était des mots. Et on entraîne le système à répondre à la question et du coup le système a une espèce de vision de ce qui se passe. Alors ça, ça ne marche pas très bien. Il y a des systèmes qui ont été produits comme ça à Meta qui ne marchent pas très bien, ou aussi à OpenAI et autres. Et puis à DeepMind, ça ne marche pas très très bien, ce n'est pas vraiment satisfaisant.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 2383.98,
      "end": 2394.56,
      "text": "Je précise une chose, vous êtes 500 chez FAIR, c'est ça? C'est ça, à peu près, oui. À faire de la recherche sur ce genre de choses. Donc il y a des équipes qui sont là-dessus, il y en a d'autres qui sont sur du LLM, il y en a d'autres qui sont sur d'autres choses.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 2386.93,
      "end": 2395.71,
      "text": "Ok, ok. C'est superbe. Alors à l'ALEM, on n'en fait plus trop.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 2395.71,
      "end": 2396.06,
      "text": "parce que...",
      "speaker": "SPEAKER2"
    },
    {
      "start": 2395.77,
      "end": 2398.98,
      "text": "parce que maintenant, c'est dans les mains d'un groupe de produits qui s'appelle Genii.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 2398.95,
      "end": 2399.03,
      "text": "OK.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 2399.03,
      "end": 2413.81,
      "text": "Donc il produit les LLM. C'est moins de la recherche, c'est du développement avancé, de la recherche appliquée on peut dire. Mais ça fait partie d'un groupe de produits qui produit Meta AI qui est le système d'IA et puis aussi les systèmes de génération d'images et d'idées.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 2400.33,
      "end": 2420.19,
      "text": "Merci. OK. Et puis... pour se rendre compte du nombre de personnes qui vont dire on arrête, on n'en fait plus. Voir un peu qui fait quoi et comment. C'est pas juste 12 personnes.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 2416.8,
      "end": 2446.2,
      "text": "Voilà. Non, donc il y a à peu près un an, un peu plus d'un an, un an et demi, l'organisation de Généré a été créée au début 2023. Et le noyau dur de R&D Engineering de Généré, en fait, c'était une soixantaine de gens de fer qui ont été déplacés dans cette organisation pour un peu amorcer la pompe. Et puis ensuite, Généré a accru. Donc maintenant, c'est une très grosse organisation avec plus de 1000 personnes.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 2446.2,
      "end": 2452.2,
      "text": "On peut se dire qu'il y a quand même une petite réaction au succès d'Open AI, si on est en janvier 2023?",
      "speaker": "SPEAKER2"
    },
    {
      "start": 2452.2,
      "end": 2456.12,
      "text": "Alors, la surprise n'a pas été technique, c'est-à-dire qu'on n'a pas été surpris par...",
      "speaker": "SPEAKER1"
    },
    {
      "start": 2455.06,
      "end": 2455.09,
      "text": "Merci.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 2456.98,
      "end": 2529.65,
      "text": "trop par les performances du système parce qu'on avait des systèmes un peu similaires mais par contre ce qui a surpris tout le monde c'est l'enthousiasme et l'engouement du public pour ça et le fait qu'il pourrait y avoir un marché, enfin quelque chose à déployer qui serait utile aux gens, des assistants. Et on prenait notre temps un petit peu parce que, justement, à cause de l'histoire de Galactica, où on s'était fait arroser de vitrioles, on s'est dit, bon, il faut peut-être faire un petit peu attention. Google était un petit peu dans la même situation. On n'avait pas vraiment besoin de ça pour justifier la recherche en IA parce que Facebook est financé par la pub, etc. Donc, il n'y a pas une grosse pression pour dériver et générer des revenus à partir de ça. motivation pour le faire parce que c'est leur seul moyen de gagner de l'argent. C'est pour ça qu'ils ont été les premiers. Ce n'est pas vraiment une avance technologique. Ils en avaient peut-être une petite, mais personne n'est en avance de qui que ce soit depuis trois mois. Il n'y a pas vraiment de secret. Tous les gens qui travaillent là-dedans se connaissent tous. La moitié des dirigeants de DeepMind sont des anciens étudiants à moi. J'ai pas mal d'anciens post-docs et de collègues à OpenAI. Tout ça, ça circule. Il n'y a pas vraiment de",
      "speaker": "SPEAKER1"
    },
    {
      "start": 2513.38,
      "end": 2518.54,
      "text": "Ok. Hum.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 2518.54,
      "end": 2524.09,
      "text": "Sous-titrage Société Radio-Canada Bonne journée.",
      "speaker": "SPEAKER0"
    },
    {
      "start": 2529.65,
      "end": 2559.65,
      "text": "recettes secrètes que personne ne connaît, ça ne dure jamais très longtemps. Mais ce qui a surpris, c'était effectivement l'engouement du public et le fait qu'il y ait peut-être un marché à déployer. Donc ça a suscité effectivement la création de l'organisation Gemini chez Google, Genia et chez Meta. Mais en fait, le résultat de ça, c'est que maintenant que Genia peut se concentrer sur le développement de produits à base de LLM et IA générative, faire,",
      "speaker": "SPEAKER1"
    },
    {
      "start": 2534.44,
      "end": 2534.72,
      "text": "OK.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 2559.65,
      "end": 2569.98,
      "text": "c'est refocaliser sur la recherche à long terme. Quelle est la prochaine génération de systèmes d'IA capables de comprendre le monde, de raisonner, planifier, etc.?",
      "speaker": "SPEAKER1"
    },
    {
      "start": 2569.98,
      "end": 2588.91,
      "text": "intéressant ce que tu dis parce que ça veut dire que pour faire en fait je caricature évidemment mais fait que lmc plier maintenant c'est parti dans une équipe produit c'est plus la recherche un peu et maintenant nous on va se concentrer sur autre chose voilà donc ce que je dis aux j'étais pas prêt à cette étude",
      "speaker": "SPEAKER2"
    },
    {
      "start": 2581.82,
      "end": 2617.14,
      "text": "Un peu, oui. Voilà, donc ce que je dis aux... Ce que je dis aux étudiants maintenant quand ils posent la question, qui veulent démarrer un doctorat ou s'intéressent à l'IA et de faire la recherche en IA, je leur dis ne travaillez pas sur les LLM. C'est le passé. C'est le passé d'une part et d'autre part c'est complètement dominé par l'industrie. Si vous n'avez pas accès à 10 000 GPU, vous n'allez jamais pouvoir faire quoi que ce soit d'utile. Si vous voulez rivaliser avec ça, même dans une start-up, il va falloir vous lever comme Mistral 500 millions ou un milliard parce que vous allez dépenser",
      "speaker": "SPEAKER1"
    },
    {
      "start": 2596.01,
      "end": 2596.15,
      "text": "Merci.",
      "speaker": "SPEAKER0"
    },
    {
      "start": 2607.18,
      "end": 2607.54,
      "text": "C'est ça.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 2617.14,
      "end": 2647.14,
      "text": "tout votre budget en calcul. En puissance de calcul. Exactement. On a cherché Nvidia. Voilà. Ou en location en cloud. Et vous allez faire face à Meta qui a 300 000 GPU, à Microsoft qu'on a acheté une quantité similaire et puis à Google qui a son propre hardware, le TPU. Donc, ça va être difficile. Il faut vraiment que vous soyez vraiment sûr que vous êtes supérieur au niveau",
      "speaker": "SPEAKER1"
    },
    {
      "start": 2619.59,
      "end": 2627.22,
      "text": "On a cherché Nvidia Ou en location, en cloud, voilà.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 2647.14,
      "end": 2661.49,
      "text": "Vous êtes électorat pour révéler là dedans. C'est un peu compliqué, mais donc essayez de travailler plutôt sur des nouveaux concepts. Essayez d'inventer l'architecture qui de nouveaux systèmes d'IA qui vont dépasser les limitations actuelles.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 2661.76,
      "end": 2687.26,
      "text": "Donc là, on a repris la feuille blanche, quasiment. Le LLM, il existe, il nous impressionne toujours, il va faire des trucs bien. Je pense que ce qui est intéressant, ce qu'on a appris avec OpenAI et le grand public, c'est peut-être des nouveaux cas d'usage qu'on n'avait peut-être pas prédits. Une fois qu'on donne ça à tout le monde, on découvre que des gens l'utilisent d'une manière... Donc ça, je trouve ça assez cool et assez intéressant. Et c'est vrai que ça devient quand même un outil...",
      "speaker": "SPEAKER2"
    },
    {
      "start": 2665.66,
      "end": 2681.27,
      "text": "– Oui, tout à fait. Euh...",
      "speaker": "SPEAKER1"
    },
    {
      "start": 2687.43,
      "end": 2715.47,
      "text": "pour moi, pour beaucoup, j'étais avec Annabelle Brouran ce week-end, je la voyais faire ses recherches sur ChatGPT 4.0. Ça devient un outil où beaucoup l'utilisent, comme Google, comme les mails, comme plein de choses. Donc ça, c'est intéressant. Avant de revenir sur cette page blanche, c'est plus chez toi, j'ai compris, c'est plus chez Fer, mais c'est donc, comment il s'appelle cette équipe? Gen AI. Gen AI.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 2699.74,
      "end": 2713.97,
      "text": "Oui. Génial.",
      "speaker": "SPEAKER0"
    },
    {
      "start": 2716.3,
      "end": 2723.49,
      "text": "C'est quelque chose que vous avez choisi de ne pas ouvrir complètement au grand public comme Gemini ou OpenAI?",
      "speaker": "SPEAKER2"
    },
    {
      "start": 2723.32,
      "end": 2753.32,
      "text": "C'est Lama. C'est même beaucoup plus ouvert. Il y a deux choses. Il y a Lama qui est un système open source, donc un LLM open source. Donc là, en fait, Lama a complètement démarré tout un écosystème autour de l'IA qui permet à des petites et grosses entreprises ou même des associations d'affiner des LLM et de les utiliser pour leurs besoins, leur application verticale, leurs clients.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 2723.64,
      "end": 2740.35,
      "text": "C'est complètement disponible. Ok.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 2753.32,
      "end": 2783.32,
      "text": "etc. Et ça, c'est rendu possible par les plateformes open source, c'est-à-dire que vous pouvez télécharger Lama, donc c'est Lama 3 qui est le dernier. Mais le déclencheur de tout ça a été la distribution de Lama 2 qui s'est déroulée l'été dernier, l'été 2003, qui a permis en fait d'essaimer complètement l'écosystème de l'IA qu'on voit par exemple à Paris où il y a énormément de startups qui ont démarré dans l'IA. Une grande partie utilise Lama comme",
      "speaker": "SPEAKER1"
    },
    {
      "start": 2753.88,
      "end": 2771.58,
      "text": "Ok. OK.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 2779.83,
      "end": 2779.97,
      "text": "– Sous-titrage Société Radio-Canada",
      "speaker": "SPEAKER0"
    },
    {
      "start": 2783.32,
      "end": 2813.32,
      "text": "comme Baz ou Mistral, qui est aussi open source, ou quelques autres. Et ce que permettent les outils open source comme ça, donc on peut télécharger l'AMA ou Mistral et l'affiner, l'ajuster. Avec ses propres données à soi. Avec ses propres données qui peuvent être privées ou pas. Avec ses fonds linguistiques, culturels, etc. Donc ça permet, par exemple, à des pays comme l'Inde de dire on va affiner l'AMA2 pour qu'il parle les 22 langues.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 2799.56,
      "end": 2799.58,
      "text": "Sous-titrage Société Radio-Canada",
      "speaker": "SPEAKER0"
    },
    {
      "start": 2799.58,
      "end": 2800.88,
      "text": "Avec ses propres données à ce point.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 2813.32,
      "end": 2873.32,
      "text": "officielle de l'Inde, ce que les systèmes de base entraînés en Californie ne font pas, ou entraînés à Paris en l'occurrence pour l'AMMA, ou ce qui permet à des gens, par exemple un ancien collègue de FAIR, qui s'appelle Moustapha Sissé, qui était chercheur à FAIR, il y a quelques années, il est reparti en Afrique, et depuis un an, il a créé une entreprise qui s'appelle Kera Health, et qui utilise des LLM open source, ajustées pour parler, pas seulement français, mais aussi le Wolof, qui est une des langues dominantes au Sénégal, et qui permet l'accès à de l'information médicale. C'est très difficile d'avoir un rendez-vous avec un docteur au Sénégal, parce qu'il n'y a que 5 docteurs pour 100 000 habitants, et puis ils sont tous dans les grandes villes, donc si on est dans un village, on n'a pas accès. Mais par contre, on pourrait bien parler à un docteur, avoir l'information sur des symptômes, etc. Donc on peut parler à un LLM comme ça, qui a été réentraîné pour parler le Wolof et donner l'information.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 2856.23,
      "end": 2873.34,
      "text": "Ok. Dans un village. Merci.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 2873.32,
      "end": 2873.71,
      "text": "médicales.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 2873.78,
      "end": 2884.37,
      "text": "Sachant que là-dessus, souvent les grands américains évitent de faire ce genre de choses parce qu'ils ont peur des responsabilités que prend l'égal sur « Oulala, je ne veux pas te faire un diagnostic parce que sinon… »",
      "speaker": "SPEAKER2"
    },
    {
      "start": 2884.37,
      "end": 2888.09,
      "text": "Exactement, mais ce n'est pas quelque chose que Meta ferait, mais par contre...",
      "speaker": "SPEAKER1"
    },
    {
      "start": 2888.09,
      "end": 2890.06,
      "text": "Si tu le fais toi-même, c'est ton problème.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 2890.06,
      "end": 2904.81,
      "text": "Avoir ces moteurs open source disponibles permet à tous ces gens d'être très inventifs sur les utilisations des LLM. Même chose en France, il y a tout un tas d'entreprises qui utilisent ces LLM open source. Donc ça c'est très différent.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 2890.08,
      "end": 2912.13,
      "text": "Voilà. – Oui, c'est intéressant. Par biais d'API, là-dessus on n'est pas forcément sur quelque chose qu'on utilise, Lama comme OpenAI.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 2912.55,
      "end": 2936.57,
      "text": "Non, alors Lama, c'est pour des gens qui sont dans le développement d'applications, des développeurs, des chercheurs, etc. Au-dessus de ça, il y a un produit développé par GenAI, donc Amita, qui s'appelle MetaAI, et qui est un agent intelligent avec qui on peut dialoguer à travers WhatsApp, Messenger ou Facebook, ou même Instagram.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 2915.61,
      "end": 2924.84,
      "text": "Dans le développement d'applications. Oui.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 2937.01,
      "end": 2937.02,
      "text": "Sous-titrage Société Radio-Canada",
      "speaker": "SPEAKER1"
    },
    {
      "start": 2937.02,
      "end": 2937.31,
      "text": "D'accord.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 2937.06,
      "end": 2937.33,
      "text": "Merci. Sous-titrage Société Radio-Canada",
      "speaker": "SPEAKER1"
    },
    {
      "start": 2937.33,
      "end": 2937.34,
      "text": "Sous-titrage Société Radio-Canada",
      "speaker": "SPEAKER2"
    },
    {
      "start": 2937.34,
      "end": 2939.99,
      "text": "Et c'est disponible si on va sur WhatsApp.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 2939.35,
      "end": 2945.66,
      "text": "sur Whatsapp je peux l'ajouter et puis parler avec lui comme sur sur sur JGPT",
      "speaker": "SPEAKER2"
    },
    {
      "start": 2942.73,
      "end": 2975.66,
      "text": "Voilà. Et sur Facebook, c'est même plus amusant, il y a des personnalités différentes de ce LLM. Par exemple, il y a un Dungeon Master, si on veut, joué à Dungeons & Dragons, qui est personnalisé par Snoop Dogg. Pourquoi Snoop Dogg, je ne sais pas. Parce qu'il est cool, c'est suffisant. Voilà. Qui est basé sur la MAD2, qui a été affinée pour être expert en Dungeons & Dragons. Donc tout un tas de choses comme ça. Et puis,",
      "speaker": "SPEAKER1"
    },
    {
      "start": 2961.07,
      "end": 2966.08,
      "text": "mais Donc il est cool, c'est suffit en fait? Voilà. Oui.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 2975.66,
      "end": 3005.66,
      "text": "ce système Meta AI est disponible à travers les lunettes connectées intelligentes de Meta, donc les Ray-Ban Meta que je porte en ce moment sur le nez. C'est mes lunettes normales en fait, de correction de vue. Et qui foncent au soleil. Qui ont des caméras et donc qui sont connectées à un LLM, on peut lui parler, lui poser n'importe quelle question. Et puis on peut même lui demander de regarder la scène qu'on est en train de regarder et nous dire, nous donner des commentaires.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 2984.43,
      "end": 2991.18,
      "text": "Ok. fait, ce correcteur de vie.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 3005.66,
      "end": 3035.66,
      "text": "c'est-à-dire, je ne sais pas, on regarde un menu en japonais, on peut lui demander... Et là, il va te parler, tu as des petits écouteurs... Des petits écouteurs par conduction par le squelette, en fait, on ne sait pas, on peut entendre de l'extérieur, on peut jouer de la musique, etc. Trop bien. Et à terme, d'ici un an ou deux, il y aura des lunettes de ce type-là avec un afficheur dans les lunettes qui permettra, par exemple, en temps réel, de produire des sous-titres si quelqu'un nous parle dans une langue",
      "speaker": "SPEAKER1"
    },
    {
      "start": 3009.52,
      "end": 3016.89,
      "text": "Et là, il va te parler, t'as des petits écouteurs... sur Donc c'est en entendant.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 3035.66,
      "end": 3041.04,
      "text": "on ne comprend pas, on a les sous-titres automatiquement avec un délai de deux secondes ou quelque chose comme ça. Donc,",
      "speaker": "SPEAKER1"
    },
    {
      "start": 3040.91,
      "end": 3044.55,
      "text": "On connaît des gens à la com de Facebook, on va voir si on peut s'en procurer.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 3044.52,
      "end": 3074.52,
      "text": "Voilà, mais ce n'est pas pour tout de suite. La technologie est encore... Et puis, on pourrait interagir aussi avec ces systèmes-là, avec des bracelets, ce qu'on appelle des bracelets électromyographiques. Donc, ça veut dire que ce sont des bracelets qu'on porte sur les poignets, comme c'est normal, qui ont des capteurs électriques qui capturent les courants électriques produits par les nerfs pour commander les muscles. Les muscles de la main ne sont pas dans la main, ils sont dans le bras. Et donc, avec ça, on peut inférer la position de la main. Et donc, on peut, en bougeant le pouce sur l'intermédiaire,",
      "speaker": "SPEAKER1"
    },
    {
      "start": 3058.12,
      "end": 3058.47,
      "text": "C'est normal. Sous-titrage Société Radio-Canada",
      "speaker": "SPEAKER0"
    },
    {
      "start": 3067.06,
      "end": 3067.5,
      "text": "Ok.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 3074.52,
      "end": 3093.56,
      "text": "index, bouger un pointeur par exemple, comme une souris, et puis cliquer. Et même, on peut taper au clavier avec les mains dans les poches. Donc ça permettra un nouveau type d'interaction en fait avec les systèmes. Et à terme, toutes les interactions de tout un chacun avec le monde numérique se feront par l'intermédiaire d'un agent intelligent.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 3094.13,
      "end": 3104.1,
      "text": "C'est ce que fait... du coup ça me fait penser à Apple et son casque là, quand tu cliques ou que tu fais ces choses là, c'est assez impressionnant quand tu l'essayes, tu l'essayais non? Je sais pas.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 3104.19,
      "end": 3108.86,
      "text": "Non mais ça il a chassé Meta depuis deux ans, c'est MetaQuest 3, Apple...",
      "speaker": "SPEAKER1"
    },
    {
      "start": 3108,
      "end": 3114.21,
      "text": "Apple essaie de grignoter, de rattraper leur retard et ils sortent",
      "speaker": "SPEAKER0"
    },
    {
      "start": 3109.89,
      "end": 3120.79,
      "text": "Apple essaie de... de rattraper et il sort un truc d'abord qui est moins bon et qui est 7 fois plus cher donc non je suis pas impressionné du tout",
      "speaker": "SPEAKER1"
    },
    {
      "start": 3121.11,
      "end": 3127.93,
      "text": "Comment il s'appelle? Evidemment, vous faites ça depuis assez longtemps. Quest 3, c'est le dernier. Ok.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 3125.72,
      "end": 3145.18,
      "text": "Le Quest 3, c'est le dernier. Et puis, il y a eu le Quest Pro avant et puis le Quest 2, bien sûr. Mais le Quest 3 est vraiment impressionnant au niveau réalité virtuelle, réalité mix. Et effectivement, interaction, on n'a pas besoin de manettes. On peut simplement avoir les mains, le système à faire la position des mains. C'est top.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 3145.18,
      "end": 3154.05,
      "text": "on ne trouve pas encore son marché là-dessus mais si ça se trouve il y a un open AI qui va débarquer qui va trouver votre marché avant vous mais bon tout le monde en profite derrière finalement le soin est disponible",
      "speaker": "SPEAKER2"
    },
    {
      "start": 3152.72,
      "end": 3156.74,
      "text": "Le 3 est disponible, ça coûte 400 euros, je ne sais pas quoi.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 3156.74,
      "end": 3157.92,
      "text": "On va essayer ça.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 3157.02,
      "end": 3182.89,
      "text": "Les lunettes intelligentes, c'est 300 euros. Alors la connexion avec MetaEye, le système LLM Intelligent, n'est pas encore disponible en France ou en Europe. Pour l'instant, c'était déployé, je crois, aux États-Unis, dans quelques pays anglophones. Mais il y a des questions de régulation, en fait, pour le déploiement. On est obligé de s'assurer qu'on respecte la législation en vigueur en Europe. C'est plus compliqué.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 3182.05,
      "end": 3196.59,
      "text": "C'est peut-être chose de la vie privée. C'est vrai que quoi qu'il arrive, tu te dis quand j'ai un bracelet, des lunettes et quelque chose avec un géant américain, je ne vais pas te faire rentrer là-dedans, mais tu peux avoir une petite angoisse et te dire...",
      "speaker": "SPEAKER2"
    },
    {
      "start": 3196.49,
      "end": 3200.32,
      "text": "On a ça avec les téléphones portables déjà, les smartphones sont connectés.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 3198,
      "end": 3203.02,
      "text": "Les smartphones sont connectés. Beaucoup de gens ont déjà ces angoisses d'ailleurs, mais...",
      "speaker": "SPEAKER2"
    },
    {
      "start": 3202.33,
      "end": 3208.34,
      "text": "– C'est ça, c'est ça. Google ou Apple, ça, à chaque seconde, exactement où on est par GPS. Enfin bon, c'est pas nouveau.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 3208.34,
      "end": 3215.88,
      "text": "C'est vrai. Et d'ailleurs, beaucoup des services, pas que Google et Apple, les gens qui ont des applis, si tu as autorisé, tu en as plein dedans. Voilà.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 3213.79,
      "end": 3213.98,
      "text": "Voilà.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 3217.5,
      "end": 3251.22,
      "text": "Avant de revenir sur cette page blanche, j'ai une question pour un dev qui n'est pas un chercheur, qui fait du dev, je ne sais pas moi, sur du front du bac, qui voudrait en fait se mettre à fond, mais déjà un peu, pour utiliser l'AMA 2, l'AMA 3 et faire des tas de trucs de ouf. Qu'est-ce qu'on lui recommande de faire? Comment il apprend? Est-ce qu'il faut faire un MOOC NYU? Est-ce qu'il y a des ressources chez vous? Est-ce que ça s'apprend tout seul sur YouTube? Qu'est-ce qu'il faut faire?",
      "speaker": "SPEAKER2"
    },
    {
      "start": 3251.22,
      "end": 3268.62,
      "text": "Si on veut apprendre vraiment les bases du deep learning, par exemple, il y a tout un tas de cours disponibles gratuitement en ligne, y compris mon cours de deep learning à NYU. Il y a la version 2020-2021. La version 2020, je crois, a été traduite en 13 langues, donc c'est même disponible en français. C'est pas",
      "speaker": "SPEAKER1"
    },
    {
      "start": 3253.55,
      "end": 3269.12,
      "text": "Oui. Très bien. C'est pas périmé.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 3269.12,
      "end": 3299.12,
      "text": "Ce n'est pas périmé. Il y a peut-être un peu moins sur les transformers et les LLM que les dernières versions. Mais sinon, il y a des blogs ou des publications séparées qui expliquent vraiment bien ce que c'est. C'est très accessible sur comment fonctionnent le deep learning, la rétrogradation de gradient, les transformers, les LLM, etc. Donc, beaucoup de choses qui sont disponibles. Ensuite, il y a des bibliothèques logicielles",
      "speaker": "SPEAKER1"
    },
    {
      "start": 3281,
      "end": 3281.88,
      "text": "C'est accessible?",
      "speaker": "SPEAKER0"
    },
    {
      "start": 3299.12,
      "end": 3385.59,
      "text": "qu'on peut utiliser, qui sont relativement faciles à utiliser, dont on n'a pas vraiment besoin de comprendre tous les détails pour arriver à les utiliser, sous PyTorch. PyTorch est un outil de développement de deep learning qui a été produit par Meta, qui n'appartient plus à Meta. Meta a transféré la propriété à la fondation Linux, donc c'est vraiment un projet communautaire. Et c'est universellement utilisé, excepté par quelques personnes à Google qui utilisent leur truc. mais c'est vraiment universellement utilisé dans l'industrie et dans la recherche surtout. Et alors si on veut appliquer simplement LLM préentraîné comme LAMA, on peut télécharger LAMA 3 ou LAMA 2 ou Mistral ou un autre. Et beaucoup de gens en fait ont développé des moyens de déployer ces LLM sur des ordinateurs qui n'ont pas une dizaine de GPU de haute puissance qui coûtent une fortune mais en fait de les faire tourner même sur des ordinateurs portables ou sur des ordinateurs de disons de taille raisonnable ou avec un GPU de gamer en utilisant des techniques de compression en fait de ces réseaux là donc on peut les rendre plus efficaces après entraînement il y a plein de trucs comme ça donc il y a un programme qui s'appelle lama.cpp qui permet de faire ça de faire tourner",
      "speaker": "SPEAKER1"
    },
    {
      "start": 3369.36,
      "end": 3369.66,
      "text": "OK.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 3385.59,
      "end": 3415.59,
      "text": "Lama sur des ordinateurs raisonnables. Sinon, si on ne veut pas rentrer dans ces détails, on peut utiliser Lama et autres à travers une API ou à travers un site qui sert de serveur. Donc, on ne le fait pas tourner chez nous, on le fait tourner chez un service cloud. Le plus simple, c'est probablement de passer par Hugging Face, qui est une entreprise franco-américaine qui",
      "speaker": "SPEAKER1"
    },
    {
      "start": 3406.84,
      "end": 3412.39,
      "text": "Ok. Clément, que j'ai reçu ici.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 3415.59,
      "end": 3474.05,
      "text": "qui offre ce genre de service. Donc tous les modèles ouverts sont disponibles sur Gameface et puis ils offrent un service qui est basé sur Amazon ou Google pour les faire tourner en remote. Ou on peut aussi s'adresser directement à Amazon, à Azure chez Microsoft ou même à GCP chez Google pour faire tourner la main sur leur cloud. C'est déjà installé, on n'a pas besoin de se prendre la tête. travers une API. Donc Amazon, par exemple, offre Lama et tous les autres systèmes à travers une API, pareil pour Azure. Donc c'est relativement simple en fait, et puis on peut construire des applications de manière relativement simple autour de ça. Puis si on veut quelque chose de vraiment customisé, on est une entreprise, on voudrait un LLM qui connaisse tous nos documents internes, qui puisse répondre à n'importe quelle question à nos employés. À ce moment-là, on peut employer les services d'une startup dont c'est la spécialité d'affiner un LLM pour l'application verticale.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 3473.94,
      "end": 3526.45,
      "text": "On a reçu là-dessus notre ami il y a quelques mois, tu peux m'aider, qui aide les entreprises qui est venue avec nous à Monaco. Je vois que tu ne m'aides pas. Je vais y revenir. Et un super épisode aussi dans lequel on est. Merci en tout cas de ces précisions. Donc je reviens sur cette page blanche. On se dit finalement, le LLM c'est un produit commercial, moi je fais de la recherche, compris que le texte, le langage n'était pas suffisant. Si je comprends bien, une des pistes, il y en a peut-être plusieurs, mais que vous travaillez avec JEPA notamment, il y a une lettre à coller, JEPA, qui est de dire en fait plutôt que de masquer des mots, on va masquer des images dans des vidéos pour comprendre le monde, c'est ça un peu?",
      "speaker": "SPEAKER2"
    },
    {
      "start": 3518.64,
      "end": 3552.39,
      "text": "J'ai pas, oui. Sous-titrage Société Radio-Canada Oui, c'est une vieille idée en fait. Donc l'idée d'utiliser l'apprentissage auto-supervisé pour permettre aux machines de comprendre comment fonctionne le monde. C'est un petit peu la même idée que pour entraîner les LLM où on prend un texte et on masque certains mots et puis ensuite on entraîne un système à prédire les mots qui manquent. On pourrait faire la même chose avec la vidéo ou les images. Donc on peut prendre des images, masquer certaines parties de l'image, entraîner un gros réseau de neurones à reconstruire l'image complète à partir d'une image partielle.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 3553.06,
      "end": 3564.49,
      "text": "C'est Stan Pollu de Dust, tu dois le connaître. Stan, tu m'excuses, on t'embrasse, on t'adore en plus, mais j'ai eu un gros bug à moi-même, tu vois ma mémoire de l'LM. On y est au camp.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 3555.04,
      "end": 3567.49,
      "text": "Ah oui, bien sûr. l'élème. En anglais, on appelle ça senior moment.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 3568.2,
      "end": 3571.14,
      "text": "Un moment de seniorité.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 3569.03,
      "end": 3607.47,
      "text": "Senior. Senior même. Quand on est trop vieux, l'hippocrampe se durcit un peu. Je ne t'insulte pas, mais je n'en pense pas moi. Donc, on pourrait imaginer ça pour les images. On pourrait imaginer ça pour la vidéo. Et en fait, sur la vidéo, on montre par exemple une vidéo, un système, et puis on arrête la vidéo. Et on lui demande de prédire ce qui va se passer après. Alors évidemment, si par exemple, la vidéo, c'est quelqu'un qui prend un stylo, le stylo est tenu au dessus de la table et la personne ouvre les doigts le stylo va tomber on peut prédire que le stylo va tomber",
      "speaker": "SPEAKER1"
    },
    {
      "start": 3605.75,
      "end": 3609.04,
      "text": "Hmm. a priori, ne va pas s'envoler.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 3607.69,
      "end": 3726.65,
      "text": "et oui, voler. Il ne va pas s'envoler, il ne va pas flotter dans l'air, il va tomber, on ne peut pas exactement prédire comment, comment il va rebondir, est-ce qu'il va se casser la mine et autres, mais on peut prédire qu'il va tomber sur la table. L'idée c'est peut-être qu'on pourrait faire la même chose qu'avec le texte, c'est-à-dire entraîner une machine à prédire ce qui va se passer dans une vidéo et ce faisant peut-être ils vont réussir à comprendre que le monde est tridimensionnel, qu'il est composé d'objets, que les objets obéissent à la physique, qu'il y a des objets animés et inanimés, etc. Et un petit peu comprendre comment fonctionne le monde à la manière des enfants et des bébés animaux, des chiens, des chats, des rats, etc., des oiseaux. Et ça ne marche pas. Et ça fait dix ans qu'on essaye. Et ça ne marche pas du tout. Ça marche très bien pour le texte et ça ne marche pas pour l'image ni la vidéo. Ça marche un peu quoi, ça marche hot. On a des papiers là-dessus qui remontent à 2014, donc ça fait dix ans. Ce n'est pas une nouvelle idée. Et puis c'est un concept qui est assez répandu dans les neurosciences qui s'appelle Predictive donc Codage pas Prédiction, dans lequel effectivement, le cerveau peut-être représente le monde en essayant de faire de la prédiction et puis en encodant les erreurs de prédiction, c'est-à-dire ce qui n'est pas prédit est surprenant et ce qui est surprenant est intéressant donc c'est une bonne manière de coder le monde en fait. Donc ces concepts sont très anciens mais on n'a pas réussi à les faire marcher et on ne peut pas les faire marcher Merci. avec des architectures génératives, c'est-à-dire que ce qu'on fait pour le texte, c'est-à-dire prédire le prochain mot dans un texte ou les prochains mots dans un texte, on peut le faire parce que le texte c'est simple et parce que c'est discret et parce qu'il n'y a qu'un nombre fini de possibilités d'un mot. La phrase que j'utilisais précédemment, le chat pourchasse le blanc dans la cuisine, il n'y a que quelques mots qui sont possibles là-dedans et on peut produire un vecteur de probabilité qui dit probablement",
      "speaker": "SPEAKER1"
    },
    {
      "start": 3703.25,
      "end": 3715.53,
      "text": "Merci. Hmm.",
      "speaker": "SPEAKER0"
    },
    {
      "start": 3726.65,
      "end": 3763.51,
      "text": "un spot laser, un jouet, etc. Et... Mais on ne peut pas faire ça avec les images. C'est-à-dire que si je montre une vidéo et je demande à un système prédit ce qui va se passer dans une seconde, donne-moi une image qui représente ce qui s'est passé dans une seconde. Ce n'est pas vraiment possible. On n'a pas une manière de représenter une distribution de probabilités sur toutes les images possibles. C'est impossible à faire. C'est un problème complètement intractable mathématiquement sur lequel les mathématiciens, les physiciens et les informaticiens et les statisticiens se cassent la tête depuis un siècle. Donc ce n'est pas possible. On ne sait pas le faire.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 3763.16,
      "end": 3764.76,
      "text": "Parce qu'il y a trop de possibilités.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 3763.51,
      "end": 3866.75,
      "text": "Trop de possibilités. Trop de possibilités, exactement. Et on ne peut pas se permettre de représenter une possibilité, c'est-à-dire d'entraîner le système à faire une seule prédiction, parce que ce qu'il va faire, c'est qu'il va prédire une espèce de moyenne de toutes les choses qui sont possibles. Et donc, si c'est l'exemple du crayon qu'on laisse tomber sur la table, le crayon peut, à la fin, être dans tout un tas de positions et il va prédire une espèce de truc flou qui est une moyenne de tous les crayons. dans toutes les positions possibles sur la table. Donc ça ne marche pas du tout. Donc on a essayé de résoudre ce problème-là avec des tas de... de... de... système D, quoi, mais qui ne marche pas. Et puis il y a quatre ans environ, euh... J'ai complètement changé d'avis sur la question avec certains de mes collègues. Et ce qu'on a réalisé, c'est que ce qui marche dans l'image et donc ce qui commence à marcher dans la vidéo, ce sont des architectures non génératives. C'est-à-dire que le problème, c'est qu'on ne peut pas produire dans tous les détails, tous les pixels qui représentent la position du crayon sur la table. Mais par contre, on peut dire que le crayon va tomber et il va être à plat sur la table. Mais ça, c'est une représentation abstraite de la réalité. Donc on ne peut pas prédire tous les pixels dans la vidéo, mais on peut peut-être prédire une représentation abstraite du contenu de la vidéo. Donc l'idée de JEPAS, JEPAS ça s'écrit J-E-P-A, et ça veut dire Joint Embedding Predictive Architecture. Comment on pourrait traduire ça en français? Architecture prédictive à enchassement joint. Je pense que ce serait le... Donc A-P-E-J.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 3867.02,
      "end": 3868.51,
      "text": "Enchassement voulant dire?",
      "speaker": "SPEAKER2"
    },
    {
      "start": 3868.47,
      "end": 3869.64,
      "text": "Enchassement et embedding.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 3869.64,
      "end": 3873.28,
      "text": "Unbedding, ok, très bien, je le comprends bien en anglais pour le coup. Voilà.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 3869.69,
      "end": 3952.05,
      "text": "Merci. et bien Très bien, on va aller déchirer le concours. le Voilà, donc qu'est-ce que ça veut dire embedding? Ça veut dire représenter une entrée, que ce soit un texte, une image, etc. en une liste de nombres qu'on peut voir comme un point dans un espace de haute dimension. Si on connaît un peu l'égypte linéaire, les espaces vectoriels. Donc la notion d'embedding en mathématiques ou dans le chassement, c'est comment représenter des objets quels qu'ils soient par des points dans des espaces de haute dimension. Alors, qu'est-ce que ça veut dire le joint embedding? Ça veut dire que si on prend une entrée, disons une image, et on lui fait subir une transformation ou une corruption, on va appeler l'image Y, on va l'appeler l'image corrompue ou transformée X. une architecture générative voudrait reproduire Y à partir de X. C'est-à-dire faire une prédiction de Y, de tous les détails, tous les pixels, à partir de X. Ça marche pour le texte, pas pour l'image. Ce qui marche pour l'image, c'est qu'on prend Y et on le passe par un encodeur, donc un système qui va calculer justement un enchassement, un embedding, une représentation abstraite de l'image qui va éliminer plein de détails sur l'image, mais qui va garder la substantifique moelle du contenu de l'image. C'est le cas.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 3951.38,
      "end": 3955.97,
      "text": "C'est le crayon qui tombe mais tout ce qui est derrière qui n'est pas important ou tout ce qui est…",
      "speaker": "SPEAKER2"
    },
    {
      "start": 3955.65,
      "end": 3985.65,
      "text": "Voilà, il tombe sur une table en bois et la texture du bois de la table n'a rien à voir avec le problème. Donc, on ne va pas représenter tout ça, tous les détails. Et puis, dans le fond, il y a, je ne sais pas, la télé qui joue un programme. On ne va pas prédire tous les pixels du programme de télé qui est en train de jouer. Donc, on va éliminer tout un tas de détails qu'on ne veut pas prédire. Et en fait, ne représenter que les choses qu'on peut prédire. Et on fait ça tout le temps. On ne s'en rend pas compte, mais en tant qu'humain et tous les animaux, on fait ça tout le temps. On ignore.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 3972.59,
      "end": 3972.71,
      "text": "Mais",
      "speaker": "SPEAKER0"
    },
    {
      "start": 3985.65,
      "end": 3990.33,
      "text": "toutes les choses qu'on ne peut pas prédire et on ne se représente en fait que les choses qu'on peut vraiment comprendre.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 3987.44,
      "end": 3999.1,
      "text": "Merci. C'est quoi que j'ai entendu? Mais on parle quand tu conduis, s'il y a du vent, tu ignores les arbres qui bougent au fond. C'est pas un truc qui te... T'es en train de conduire, tu fais pas gaffe à ça, quoi.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 3995.9,
      "end": 4002.17,
      "text": "C'est pas un truc Alors que si on avait un modèle génératif dans la tête...",
      "speaker": "SPEAKER1"
    },
    {
      "start": 4002.17,
      "end": 4002.41,
      "text": "Oui.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 4002.19,
      "end": 4006,
      "text": "Oui. il faudrait prédire le mouvement de chaque feuille.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 4006,
      "end": 4006.04,
      "text": "Merci.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 4006.04,
      "end": 4011.24,
      "text": "sur le pare-brise. Les petites vagues sur le lac. Il y a la 30",
      "speaker": "SPEAKER1"
    },
    {
      "start": 4006.09,
      "end": 4011.93,
      "text": "sur le pare-brise. La 38ème goutte, là.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 4011.93,
      "end": 4012.08,
      "text": "les",
      "speaker": "SPEAKER1"
    },
    {
      "start": 4012.08,
      "end": 4012.53,
      "text": "pas de...",
      "speaker": "SPEAKER2"
    },
    {
      "start": 4012.53,
      "end": 4012.82,
      "text": "Tu vois?",
      "speaker": "SPEAKER1"
    },
    {
      "start": 4012.82,
      "end": 4015.99,
      "text": "Oh my god",
      "speaker": "SPEAKER2"
    },
    {
      "start": 4012.86,
      "end": 4072.86,
      "text": "Vous ne l'avez pas prévu? Oh my god. Donc l'idée, c'est élever, en fait, trouver une représentation abstraite du monde dans lequel les détails imprédictibles sont éliminés, de manière à ce que le système puisse faire de la prédiction, mais que des choses importantes. Il y a un peu l'analogie dans la science, dans la physique par exemple. Qui aurait pu penser au XVIIe siècle que pour prédire la trajectoire des planètes, La seule chose qu'il suffit de savoir, c'est les trois coordonnées de position de la planète et trois valeurs de vitesse, de vélocité. Avec ces six quantités, on peut complètement prédire la trajectoire de la planète. C'est-à-dire la taille, la forme, la masse, la couleur, la densité de la planète, tous les détails sur la planète, s'il y a des êtres vivants ou pas, tout ça n'a aucune importance. La seule chose qu'il suffit de savoir, c'est",
      "speaker": "SPEAKER1"
    },
    {
      "start": 4055.48,
      "end": 4055.82,
      "text": "Cité?",
      "speaker": "SPEAKER0"
    },
    {
      "start": 4072.86,
      "end": 4084.25,
      "text": "ces six paramètres. Donc ça, en fait, c'est cette idée de comment élaborer une représentation abstraite du monde qui nous permet de faire des prédictions Et savoir ce qui est utile et inutile.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 4084.57,
      "end": 4108.71,
      "text": "Ça c'est vertigineux parce que, en fait, tu vois, tout à l'heure, on ne comprend pas forcément tout quand tu parles d'un bébé ou d'un... En fait, le bébé, je pense que les premières fois qu'il reçoit des gouttes sur la tête, tu vois, il va dire une goutte, tiens, et puis la deuxième fois, et puis à la fin, il va dire un ballon, c'est mieux, tu vois, j'exagère, mais tu vois, il y a le truc qui est au centre de son attention et tout le reste est périphérique. Et ça, ça vient petit à petit...",
      "speaker": "SPEAKER2"
    },
    {
      "start": 4106.26,
      "end": 4139.13,
      "text": "Oui, absolument. Et on est biaisé, enfin la nature humaine et puis pour les animaux c'est pareil, notre cerveau est construit pour prêter attention à certaines choses et pas d'autres. C'est-à-dire qu'on est, les bébés humains prêtent attention beaucoup à ce qui bouge, parce qu'en général il y a des choses plus intéressantes dans ce qui bouge, ils prêtent attention aux choses qui sont surprenantes, c'est-à-dire que le cerveau prédit constamment ce qui va se passer et dès qu'une prédiction est violée, on est obligé d'y prêter attention parce que ça veut dire que notre modèle du monde était faux. Et donc il faut l'ajuster. Et en fait c'est comme ça",
      "speaker": "SPEAKER1"
    },
    {
      "start": 4137.11,
      "end": 4137.36,
      "text": "Hum.",
      "speaker": "SPEAKER0"
    },
    {
      "start": 4139.13,
      "end": 4169.13,
      "text": "on sait que les bébés ont intégré certaines caractéristiques du monde. Si on montre à un bébé de 6 mois un jouet, une petite voiture sur une plateforme, donc il se repose sur une plateforme. Bon, le bébé de 6 mois voit un jouet bien coloré, il regarde un petit peu, mais pas trop. On pousse le jouet de la plateforme et le jouet semble flotter dans l'air, c'est-à-dire ne tombe pas. Un bébé de 6 mois, bon, va regarder, mais sans vraiment être intéressé.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 4139.4,
      "end": 4139.7,
      "text": "Merci.",
      "speaker": "SPEAKER0"
    },
    {
      "start": 4169.13,
      "end": 4183.24,
      "text": "Parce qu'un bébé 6 mois n'a pas encore compris que les objets tombent à cause de la gravité. Ça prend 9 mois. Donc si on monte le même scénario à un bébé de 10 mois, il va écarquiller les yeux et se concentrer sur l'objet et se demander ce qui se passe en disant...",
      "speaker": "SPEAKER1"
    },
    {
      "start": 4181.74,
      "end": 4183.29,
      "text": "Hum. Merci.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 4183.29,
      "end": 4243.29,
      "text": "l'objet flotte. Pourquoi l'objet flotte? Les bébés de 9 mois ne parlent pas, mais ils ont très bien compris tout un tas de détails de la physique intuitive. Et c'est ce genre d'apprentissage qu'on voudrait pouvoir reproduire avec les machines en les entraînant sur des vidéos, en leur entraînant à essayer de prédire ce qui va se passer dans une vidéo, mais à ne pas essayer de leur faire prédire tous les détails de l'image, mais de construire une représentation abstraite dans laquelle ils peuvent faire ces prédictions. Donc ça, c'est des modèles, c'est pas, c'est Gentleman in Productive Architecture, sont non génératifs. Et ça, c'est un peu difficile à faire passer quand tout le monde, quand la mode technologique, c'est les modèles génératifs, générative AI, même l'organisation chez Meta s'appelle GNI, Gemini s'appelle Gemini parce que c'est génératif, etc. Tout le monde parle d'IA générative, et je dis aux gens, non, si on veut faire des programmes en AI, il faut abandonner l'idée de modèle génératif, ça ne peut pas marcher",
      "speaker": "SPEAKER1"
    },
    {
      "start": 4230.8,
      "end": 4230.81,
      "text": "Merci.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 4230.81,
      "end": 4231.17,
      "text": "mais j'ai",
      "speaker": "SPEAKER0"
    },
    {
      "start": 4243.29,
      "end": 4254.42,
      "text": "vidéo si on veut que les modèles comprennent le monde. Il faut utiliser ces JEPA. Et il faut arriver à les faire marcher. Donc pour l'instant, c'est là-dessus qu'on travaille. Les faire marcher sur la vidéo et d'autres choses.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 4254.72,
      "end": 4266.49,
      "text": "Et dans le principe, à parté, je viens de comprendre pourquoi je faisais marrer les enfants, les bébés et pas les adultes. C'est parce qu'il suffit de faire... Et puis ils se marrent et tu les surprends alors que ça ne marche pas.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 4264.63,
      "end": 4279.65,
      "text": "Ça marche pas. Oui, tout est surprenant pour les bébés. Par exemple, on peut jouer à... En anglais, on appelle ça « peek-a-boo ». On met ses mains devant son visage et on fait « bouh » devant les mains. Et ça fait rigoler les bébés. En fait...",
      "speaker": "SPEAKER1"
    },
    {
      "start": 4272.97,
      "end": 4273.1,
      "text": "Oui.",
      "speaker": "SPEAKER0"
    },
    {
      "start": 4274.96,
      "end": 4280.71,
      "text": "et puis on fait bouh dans le milieu. J'essaye chez mes potes",
      "speaker": "SPEAKER2"
    },
    {
      "start": 4280.71,
      "end": 4282.26,
      "text": "Sous-titrage Société Radio-Canada Non, pas trop.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 4280.98,
      "end": 4282.28,
      "text": "Il dit les combles. Merci.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 4282.28,
      "end": 4312.28,
      "text": "Mais pour les bébés, ça marche bien parce que même la notion de permanence des objets n'existe pas probablement à la naissance. C'est des choses qu'on apprend très vite dans les trois premiers mois. Mais le fait qu'un objet existe toujours, même s'il est caché, n'est pas évident. C'est appris dans les trois premiers mois de la vie. Apparemment, chez les poussins, c'est inné plus ou moins. Mais chez les humains, probablement, c'est appris.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 4287.24,
      "end": 4287.5,
      "text": "Oui.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 4312.28,
      "end": 4316.91,
      "text": "Et donc, c'est ce qui fait que... Et puis Kabu, c'est drôle.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 4317.6,
      "end": 4343.17,
      "text": "En fait, ça paraît un peu anecdotique tout ce qu'on dit là, mais quand je dis que c'est vertigineux, c'est que... En tout cas, moi, dans cet épisode et dans cet échange avec toi, Yann, je comprends très bien la profondeur de ce que l'élème ne comprend pas, en fait. Et ce qui est intéressant aussi dans ce qu'on dit là, c'est que Il n'est peut-être pas exclu pour toi que j'ai pas dans deux temps, tu l'abandonnes parce que vous disiez en fait ça marchera pas, c'est pas la bonne voie. C'est ça la recherche?",
      "speaker": "SPEAKER2"
    },
    {
      "start": 4343.17,
      "end": 4493.17,
      "text": "C'est possible. En fait, l'histoire de l'IA est jonchée de cadavres d'idées de ce type-là, où les gens se sont dit, c'est comme ça qu'il faut faire, on va poursuivre cette approche. Et puis, ils se sont aperçus au bout de quelques années que ça menait à une espèce de mur de pierre qu'on ne pouvait pas franchir. Une histoire intéressante dans l'histoire de l'IA, c'est dans les premières années de l'IA, dans les années 50, deux chercheurs qui sont des pionniers de l'IA, Nual et Simon, Herbert Simon et Alan Nual, qui étaient à l'université Carnegie Mellon, ont proposé une méthode qu'ils appelaient, en toute modestie, le General Problem Solver, donc le solutionneur de problèmes général. Ils ont dit, c'est très simple, n'importe quel problème de raisonnement, en fait, peut être formulé comme un problème de recherche d'une solution du problème. Donc, s'il y a une bonne manière, par programme de caractériser si un problème était résolu ou pas. Est-ce que vous avez gagné aux échecs? Est-ce que vous avez trouvé le plus court chemin d'une ville à une autre? Tout ça, ça peut se réduire à un problème de recherche d'une solution dans un espace de solution. Et à partir du moment où il y a une caractérisation de la solution, il suffit de rechercher la configuration qui satisfasse à l'objectif. Et donc, dans la mesure où on peut formuler n'importe quel problème de ce type-là, on peut juste écrire un programme qui va utiliser des heuristiques différentes résoudre des problèmes différents, mais à la fin, c'est tous des variations de la même chose. Donc, ils ont écrit ce programme. Puis, ils se sont dit, avec ça, on va résoudre tous les problèmes. Ce qu'ils n'avaient pas encore compris, c'est que d'abord, formuler un problème de cette manière-là, ce n'est pas toujours très simple. Et que deuxièmement, certains problèmes nécessitent justement une quantité de connaissances a priori qui est assez importante et qui est très difficile à formuler. Troisièmement, que la plupart des problèmes nécessitaient des heuristiques de recherche de solutions, spécifiques pour chaque problème. Et donc, ce n'était pas du tout général. Et puis surtout, le gros, gros, gros problème, c'est que la plupart des problèmes intéressants sont exponentiels. C'est-à-dire, la complexité de calcul qu'on doit passer croît exponentiellement avec la complexité du problème. Et donc, ça veut dire que la plupart des problèmes intéressants sont complètement insolubles et intractables. Et ça a donné lieu, en fait, à l'apparition de la théorie de la complexité en calcul, etc., qui sont les bases de l'informatique théorique. Donc, voilà. Donc, ils se sont heurtés à un mur. Et donc,",
      "speaker": "SPEAKER1"
    },
    {
      "start": 4475.26,
      "end": 4477.69,
      "text": "Hum. Oui.",
      "speaker": "SPEAKER0"
    },
    {
      "start": 4493.17,
      "end": 4517.32,
      "text": "Ça s'est un petit peu arrêté. Et puis, simultanément, il y avait des gens qui travaillaient sur justement des machines capables d'apprentissage qu'on appelle Perceptron à l'époque et puis d'autres. Et puis, ils se sont aperçus aussi au milieu des années 60 que c'était très limité. On ne pouvait pas faire des choses vraiment intéressantes avec parce qu'on ne pouvait pas entraîner des réseaux de neurones à plusieurs couches. Ça, c'est venu dans le milieu des années 80. C'est ce qu'on appelle le deep learning maintenant. Ok.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 4516.93,
      "end": 4517.33,
      "text": "Ok.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 4517.33,
      "end": 4577.99,
      "text": "Sous-titrage Société Radio-Canada et puis d'autres vagues d'intérêts dans les années 80 les gens ont construit ce qu'on appelait des systèmes experts donc des systèmes capables de raisonnement logique mais le problème de ça c'est qu'il faut spécifier toutes les connaissances a priori pour que le système soit capable de faire ce raisonnement les règles de raisonnement etc mais il y a eu une grande mode un gros engouement au début des années 80 là dessus le Japon a démarré un gros programme qui s'appelait ordinateur 5ème génération on allait construire des ordinateurs spécialisés qui allaient pouvoir faire tourner des systèmes experts, etc. Échec total. Il y a une industrie qui s'est construite autour de ça, qui a construit des outils qui sont utilisés un peu partout maintenant, mais on n'a plus la prétention de construire des machines intelligentes avec ce genre de modèle. Et puis ensuite, simultanément, aussi une vague d'intérêts pour les réseaux de neurones multicouches, parce qu'on avait trouvé une méthode qui s'appelle la rétropropagation de gradient pour entraîner des systèmes multicouches, ce qu'on ne savait pas faire dans les années 60. Et donc ça a ouvert toute une communauté qui a commencé à travailler là-dessus. Et puis au bout de 10 ans,",
      "speaker": "SPEAKER1"
    },
    {
      "start": 4551.51,
      "end": 4551.91,
      "text": "Oui.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 4577.99,
      "end": 4594.2,
      "text": "dans le milieu des années 90, c'est un peu tombé en désuétude parce que ces systèmes-là étaient difficiles à faire marcher, les ordinateurs n'étaient pas puissants, on n'avait pas beaucoup de sources de données, etc. C'était avant l'Internet. Et ces techniques-là sont réapparues, c'est ce qu'on appelle le deep learning maintenant, mais ça a pris 15 ans. Et...",
      "speaker": "SPEAKER1"
    },
    {
      "start": 4590.62,
      "end": 4600.44,
      "text": "ça me fait Donc ce sont des vagues après vagues après vagues après vagues. Toi, tu en as surfé quelques-unes finalement? Ben oui, finalement, oui.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 4595.65,
      "end": 4620.98,
      "text": "vague après vague. Et donc des vagues où on se dit ça y est maintenant on a trouvé le secret de l'intelligence. D'ici 10 ans on aura des machines aussi intelligentes que les humains. Et puis on s'aperçoit que finalement ces techniques sont limitées. Les LLM font partie de ça aussi. Il y a une vague d'intérêt maintenant qui va retomber. On va s'apercevoir leurs limitations. On s'en est aperçu déjà. Et puis la question c'est on travaille sur la prochaine vague.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 4621.22,
      "end": 4649.72,
      "text": "Est-ce que ce que tu me dis là, ce n'est pas débattu? C'est-à-dire que sur le LLM, notamment quand on voit les millions investis un peu partout, les milliards, pardon, investis. Des dizaines de milliards. Des dizaines de milliards. Alors, il y a probablement, je le disais, du fait que beaucoup de gens l'utilisent et réfléchissent autour des cas d'usage qui vont être intéressants par le LLM. Mais le fait que l'intelligence artificielle générale, je crois que tu n'aimes pas trop ce terme, mais peu importe, arrive...",
      "speaker": "SPEAKER2"
    },
    {
      "start": 4647.56,
      "end": 4647.57,
      "text": "Merci.",
      "speaker": "SPEAKER0"
    },
    {
      "start": 4647.57,
      "end": 4648,
      "text": "ou de votre...",
      "speaker": "SPEAKER1"
    },
    {
      "start": 4650.29,
      "end": 4652.72,
      "text": "par le LLM, tout le monde est d'accord, ce ne sera pas le cas.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 4652.72,
      "end": 4682.72,
      "text": "Alors non, tout le monde n'est pas d'accord. Il y a des questions un petit peu philosophiques derrière, donc des philosophes qui travaillent sur la philosophie de la connaissance, etc., l'épistémologie et autres, ou les questions de la conscience, qui disent peut-être qu'on peut construire un système intelligent entraîné purement sur le texte. On n'a peut-être pas besoin d'entrées sensorielles, disons comme la vision, pour construire un système intelligent. C'est une question conceptuelle, même philosophique, assez intéressante.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 4653.57,
      "end": 4653.95,
      "text": "Tout le monde n'est pas",
      "speaker": "SPEAKER2"
    },
    {
      "start": 4682.72,
      "end": 4712.72,
      "text": "gens qui viennent de l'IA par l'intermédiaire de la vision, en connaissance de la parole, sont tous absolument convaincus que les LLM sont insuffisants et on ne pourra pas arriver à l'intelligence humaine sans avoir une sorte de perception sensorielle, disons. Par contre, les gens qui viennent du traitement de la langue ou des linguistes, eux donnent beaucoup d'importance à la langue et pensent peut-être que c'est tout ce dont on a besoin. Certaines personnes disent oui, mais en fait, on peut traiter les images comme",
      "speaker": "SPEAKER1"
    },
    {
      "start": 4709.76,
      "end": 4710.16,
      "text": "Hmm.",
      "speaker": "SPEAKER0"
    },
    {
      "start": 4712.72,
      "end": 4820.88,
      "text": "comme la langue, ça marchera, etc. Bon, il y a eu des expériences là-dessus qui n'ont pas très bien marché. Moi, je n'y crois pas du tout. On a même des projets à faire qu'on a arrêtés, en fait, parce que c'était un petit peu dans cette direction-là et on pense que ça ne marche pas. On a eu des projets aussi, justement, d'entraînement de systèmes d'autosupervision par reconstruction, en utilisant des modèles génératifs, dans lesquels on prend une image, on fait une corruption, une transformation de l'image ou une vidéo, et on entraîne le système à reconstruire ce qui manque dans l'image. arrêté aussi parce que ça ne marche pas bien. Pour se refocaliser justement sur ces architectures de joint embedding, les JEPA, parce qu'on a beaucoup de données expérimentales qui montrent que l'apprentissage autosupervisé avec du joint embedding, ça marche bien. On peut apprendre des belles représentations. Il y a un projet d'ailleurs qui sort de Ferpari, qui s'appelle Dino, qui est utilisé par beaucoup de gens, qui permet en fait d'encoder des images, quelle que soit l'utilisation qu'on veut en faire. Si on veut faire la reconnaissance d'objets, de l'analyse des images médicales, on avait un projet même en collaboration avec des gens en externe dans lequel on prenait des c'était dirigé par Camille Coupry à faire Paris on prend des images satellitaires du monde entier Et ce qu'on voudrait, c'est pouvoir estimer la hauteur de la canopée des arbres. Parce que ça permettrait d'estimer la quantité de carbone qui est capturée dans la végétation. Et pour certaines régions du monde, on a l'information de la hauteur de la canopée parce que des avions sont passés, des radars, etc. Donc on a l'information. Donc on entraîne un système de vision artificielle à prédire la hauteur de la canopée. On n'a pas assez de données pour entraîner un système complet. Donc on fait passer les images à travers ce système d'extraction de caractéristiques.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 4795.33,
      "end": 4795.6,
      "text": "Hum.",
      "speaker": "SPEAKER0"
    },
    {
      "start": 4820.88,
      "end": 4841.81,
      "text": "qui s'appelle Dino. Et ensuite, on entraîne un petit réseau de rhône dessus à prédire la hauteur de la canopée à partir de ces représentations. Et ensuite, on peut l'appliquer à la Terre entière et avoir une estimation de la totalité du carbone capturé dans la végétation dans le monde entier. C'est un exemple d'utilisation de l'IA pour l'étude du climat.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 4841.94,
      "end": 4884.08,
      "text": "Et le LLM, ce qui est intéressant, c'est que je comprends bien, moi, tu vois, maintenant... Merci. potentiellement la limite, je ne sais pas sur quoi bosse d'ailleurs Tesla, mais tu vois, te dire si en effet, le capteur d'une voiture automatique... capte tellement d'informations non nécessaires et puis savoir laquelle est nécessaire en fait voir alors c'est vrai qu'on a vu des choses impressionnantes quand on voit sur trois voitures devant un frein mais ça semble potentiellement assez simple la lumière rouge mais si elle n'est pas rouge et si c'est un autre et voilà mais être capable de comprendre et d'avoir du discernement pour se dire ça c'est une information intéressante ça s'en est pas une en fait c'est infini oui",
      "speaker": "SPEAKER2"
    },
    {
      "start": 4865.97,
      "end": 4866.02,
      "text": "Merci.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 4866.02,
      "end": 4866.04,
      "text": "Merci.",
      "speaker": "SPEAKER0"
    },
    {
      "start": 4866.04,
      "end": 4866.09,
      "text": "Sous-titrage Société Radio-Canada",
      "speaker": "SPEAKER1"
    },
    {
      "start": 4866.09,
      "end": 4870.54,
      "text": "– Oui. Enfin, la lumière rouge.",
      "speaker": "SPEAKER0"
    },
    {
      "start": 4883.79,
      "end": 4913.79,
      "text": "Mais dans le cas de Tesla, Waymo et autres, les systèmes de conduite autonome, et puis il y en a chez Daimler-Benz qui sont faits par Nvidia, un des acteurs dominants dans le domaine est une entreprise israélienne qui s'appelle Mobileye. C'est eux qui font une grande partie des systèmes de freinage automatique, de conduite automatique sur l'autoroute, etc. Ces systèmes-là trichent un petit peu. C'est-à-dire qu'ils sont entraînés de manière supervisée en grande partie. C'est-à-dire qu'on les entraîne à détecter",
      "speaker": "SPEAKER1"
    },
    {
      "start": 4899.38,
      "end": 4899.54,
      "text": "Oui.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 4911.23,
      "end": 4911.55,
      "text": "Hum.",
      "speaker": "SPEAKER0"
    },
    {
      "start": 4913.79,
      "end": 4943.79,
      "text": "des voitures, on collecte plein d'images avec des dashcams et puis on a des gens qui disent là il y a une voiture, là il y a un piéton, là il y a un vélo, etc. Ils sont à telle distance, on peut aussi collecter des images venant de l'IDAR, c'est une espèce de radar laser qui donne la distance. Ou avec la stéréo, c'est-à-dire avec deux caméras, on peut par triangulation estimer la distance. Et ensuite on entraîne un réseau de neurones à faire ces prédictions.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 4922.52,
      "end": 4931.02,
      "text": "Merci. Non. Voilà, Zer qui dit ça.",
      "speaker": "SPEAKER0"
    },
    {
      "start": 4931.02,
      "end": 4932.26,
      "text": "la distance.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 4943.79,
      "end": 4960.81,
      "text": "En fait, en général, c'est des réseaux convolutifs. C'est un peu mon invention qui date de 35 ans et qui détecte les objets qu'on a besoin de détecter. Donc là, la tâche de savoir ce qui est intéressant ou pas, en fait, a été faite par les humains qui ont décidé de détecter certains objets. D'accord.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 4959.42,
      "end": 4962.23,
      "text": "d'étiqueter certains objets il n'y a pas de langage là-dedans",
      "speaker": "SPEAKER2"
    },
    {
      "start": 4962.23,
      "end": 4992.23,
      "text": "Donc il n'y a pas de langage du tout. Ensuite, une fois qu'on a détecté où sont les obstacles, on peut aussi essayer de faire une carte des espaces qui sont traversables par la voiture. Et ensuite, on peut essayer de planifier une trajectoire qui fait qu'on ne va pas se cogner dans un autre obstacle qui tire partie de la dynamique de la voiture, angle du volant, confort des passagers, etc. Et c'est comme ça qu'on fait des voitures autonomes.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 4974.66,
      "end": 4974.93,
      "text": "tu",
      "speaker": "SPEAKER2"
    },
    {
      "start": 4992.23,
      "end": 5022.23,
      "text": "Et c'est, on a besoin d'équipes de centaines d'ingénieurs, en fait, pour tirer parti de tous les cas particuliers. Donc, on a, par exemple, un truc qui détecte un vélo. Bon, c'est très bien. S'il y a un vélo qui est de profil, bien sûr, et qu'on a une voiture, on se rapproche, on va se cogner dans le vélo. Donc, il faut freiner. Maintenant, il y a des gens qui mettent leur vélo sur le haillon de leur voiture, qui le suspendent parce qu'ils transportent leur vélo. Que doit faire un système de pilotage automatique?",
      "speaker": "SPEAKER1"
    },
    {
      "start": 5017.46,
      "end": 5018.22,
      "text": "parce que je transmets",
      "speaker": "SPEAKER0"
    },
    {
      "start": 5021.88,
      "end": 5022.47,
      "text": "Oui, c'est vrai.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 5022.23,
      "end": 5026.84,
      "text": "Dans ce cas-là, il va avoir peur et dire « Oh my God, je vais renverser ce vélo, je dois freiner. »",
      "speaker": "SPEAKER1"
    },
    {
      "start": 5026.27,
      "end": 5048.49,
      "text": "C'est la fameuse image qui était juste un mème à mourir de rire, mais de ce type qui, je crois que c'était un mec qui réveille sa femme, il est derrière un camion qui transporte lui-même des camions, et le camion est à l'envers, et puis tout d'un coup il se met à hurler, et puis la femme elle ouvre, et puis elle dit Et en effet, tu dis, si tu vois un camion qui t'arrive dessus, ça pose un... En effet, c'est pas facile.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 5048.56,
      "end": 5138.92,
      "text": "Non, puis il y a des tas de trucs comme ça. Tesla se posait la question, est-ce que pour améliorer la fiabilité, en plus des caméras, on pourrait utiliser des radars? Les premiers Tesla avaient des radars, ce qui permet d'estimer la distance des choses devant. Mais les radars, ce n'est pas parfait parce que les radars, on est obligé d'enlever tous les échos du radar qui viennent d'objets qui sont fixes. Donc les radars ne peuvent détecter que des voitures qui bougent. Et il y a eu un accident très célèbre de Tesla où un camion, bien blanc, était en travers de la route et donc pas détectable au radar parce que le radar enlève les objets fixes qui ne bougent pas par rapport au fond et la caméra était un peu hypnotisée par le camion blanc et donc n'a pas vu le camion et la Tesla est passée sous le camion et décapitait la voiture et le conducteur. Donc ça, c'est des cas rares et qu'il faut des équipes de centaines d'ingénieurs qui collectent des milliers d'heures de données, des dizaines de centaines de milliers d'heures de données pour essayer d'avoir des événements rares comme ça et les traiter séparément et donc il Comment se fait-il qu'un adolescent peut apprendre à conduire en 20 heures? On n'a pas les techniques pour ça. C'est clairement, il nous manque quelque chose. Donc la compréhension du monde, une espèce de physique intuitive, un certain sens commun qui permet non seulement aux adolescents de 17 ans d'apprendre à conduire, mais même à un an ou autant d'apprendre à conduire.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 5140.36,
      "end": 5148.71,
      "text": "On n'a plus beaucoup de temps, alors je ne vais pas te retenir, mais moi je trouve ça passionnant, je pourrais y passer des heures encore. Il y a des choses qui te font peur dans l'IA.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 5148.71,
      "end": 5148.73,
      "text": "Sous-titrage Société Radio-Canada",
      "speaker": "SPEAKER1"
    },
    {
      "start": 5148.73,
      "end": 5148.75,
      "text": "Sous-titrage Société Radio-Canada",
      "speaker": "SPEAKER2"
    },
    {
      "start": 5150.18,
      "end": 5204.57,
      "text": "Oui, il y a une chose qui me fait peur, c'est pas trop l'IA elle-même, c'est plutôt la direction que pourrait emprunter le marché de l'IA. C'est-à-dire qu'il y a un futur, dont j'ai parlé précédemment, dans lequel toutes nos interactions avec le monde numérique se feront par l'intermédiaire d'assistants d'IA qui résideront dans nos smartphones, nos lunettes intelligentes, tous nos instruments portables. Et on va plus s'adresser à un moteur de recherche, on va juste poser les questions à notre assistant. À un certain moment dans le futur, ces assistants auront l'intelligence de niveau humain, peut-être pour certaines tâches supérieures à l'humain. Alors il ne faut pas en avoir peur, parce que travailler avec des gens plus intelligents que soi, c'est bien. J'en suis le...",
      "speaker": "SPEAKER1"
    },
    {
      "start": 5204.99,
      "end": 5206.19,
      "text": "Je n'en ai jamais fait l'expérience, malheureusement.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 5206.19,
      "end": 5236.19,
      "text": "Moi, en fait, je ne fais que ça. Je fais qu'embaucher des gens qui sont plus intelligents que moi. On peut se représenter, bien sûr, si on est, je ne sais pas, leader, directeur académique dans l'industrie ou politique, certainement politique. On travaille avec un staff qui, en général, compose des gens qui sont plus intelligents que nous ou qui ont une expertise différente de la nôtre et qui peuvent nous conseiller sur des points importants.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 5206.24,
      "end": 5206.27,
      "text": "Merci.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 5214.44,
      "end": 5214.46,
      "text": "Sous-titrage Société Radio-Canada",
      "speaker": "SPEAKER0"
    },
    {
      "start": 5214.46,
      "end": 5228.97,
      "text": "Mouti. de",
      "speaker": "SPEAKER2"
    },
    {
      "start": 5228.97,
      "end": 5229.17,
      "text": "– Sous-titrage ST' 501",
      "speaker": "SPEAKER0"
    },
    {
      "start": 5236.19,
      "end": 5266.19,
      "text": "se sentir menacé par avoir à notre service des agents intelligents, plus intelligents que nous. Au contraire, ça va amplifier notre intelligence. J'aime bien essayer de dire de manière un peu optimiste, avec des étoiles dans les yeux, que ça pourrait mener à une espèce de nouvelle renaissance, un petit peu ce qui s'est passé avec l'invention de l'imprimerie au 15e siècle, où ça a suscité un intérêt pour les gens d'apprendre à lire et puis ça a disséminé des idées",
      "speaker": "SPEAKER1"
    },
    {
      "start": 5243.94,
      "end": 5263.83,
      "text": "Ça va. C'est-à-dire,",
      "speaker": "SPEAKER0"
    },
    {
      "start": 5266.19,
      "end": 5313.87,
      "text": "la philosophie, le rationalisme, la démocratie, la science, etc. Et puis à mener à la révolution française, la révolution américaine, la destruction du système féodal, etc. C'est vraiment des révolutions importantes. Et il est possible que l'IA, grâce au fait qu'il va augmenter l'intelligence humaine, va apporter une espèce de nouvelle renaissance aussi, un renouveau en fait, un nouveau siècle des lumières. Donc ça, c'est la vision hyper optimiste. Et c'est en partie grâce au, pas forcément l'IA générative aux assistants, mais au fait que l'IA peut aider à faire progresser la science et la médecine beaucoup plus rapidement que ça a été possible jusqu'à présent. – Sous-titrage FR 2021",
      "speaker": "SPEAKER1"
    },
    {
      "start": 5313.87,
      "end": 5314.69,
      "text": "y compris les LLM.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 5315.35,
      "end": 5373.82,
      "text": "Alors les LLM peuvent aider, bien sûr, mais ce ne sont pas des systèmes, par exemple, qui prédisent le repliement des protéines pour essayer de faire la conception de nouveaux médicaments, des choses comme ça, ou de comprendre un peu les mécanismes de la vie. Ce ne sont pas des LLM. Mais par contre, les architectures qu'ils utilisent sont très similaires à celles qui sont utilisées dans les LLM, c'est-à-dire les transformeurs. Ils sont aussi entraînés de manière autosupervisée. En fait, on a fait certains travaux pionniers dans ce domaine. Donc traiter une séquence de... d'acide aminé comme une séquence de mots, c'est la même chose. Mais ces systèmes sont quand même spécialisés, c'est-à-dire que ils sont faits pour spécifiquement calculer la conformation d'une protéine, savoir si une protéine peut se coller à une autre, aider à la conception de nouvelles protéines qui se colleraient à certains sites pour traiter certaines maladies, etc. Donc beaucoup de progrès...",
      "speaker": "SPEAKER1"
    },
    {
      "start": 5348.46,
      "end": 5395.71,
      "text": "Ok Ce sont des choses qui sont orientées. Peut-être que je vais prédire un peu la suite ou pas. Tu peux l'orienter dans tes recherches sur plein d'univers. Peut-être des armes, peut-être de la gestion des populations, de la manipulation, de plein de choses en fait, non?",
      "speaker": "SPEAKER2"
    },
    {
      "start": 5388.74,
      "end": 5388.76,
      "text": "Sous-titrage Société Radio-Canada",
      "speaker": "SPEAKER0"
    },
    {
      "start": 5388.76,
      "end": 5399.79,
      "text": "Oui. Oui, alors les choses dont on pourrait avoir peur...",
      "speaker": "SPEAKER1"
    },
    {
      "start": 5395.74,
      "end": 5395.76,
      "text": "Sous-titrage Société Radio-Canada",
      "speaker": "SPEAKER2"
    },
    {
      "start": 5400.91,
      "end": 5430.91,
      "text": "arriveront ou n'arriveront pas en fonction de la force de nos institutions démocratiques. C'est-à-dire que, par exemple, on pourrait aujourd'hui, les technologies existent, mettre des caméras partout dans Paris et reconnaître les visages de tout le monde dans les lieux publics. Et fliquer les gens. Ça existe dans certains pays autoritaires. En France, bien sûr, ce serait une violation complète des lois sur la vie privée. Dans le AI Act de l'Union européenne, il y a une loi spécifique",
      "speaker": "SPEAKER1"
    },
    {
      "start": 5417.13,
      "end": 5418.14,
      "text": "qui existent dans certains pays.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 5430.91,
      "end": 5460.91,
      "text": "contre ça. On pourrait utiliser l'IA aussi pour des scores sociaux, qui est un petit peu en Chine, pour tout un tas de systèmes d'invasion, en fait, des libertés individuelles. Mais on a des institutions démocratiques assez fortes qui vont empêcher ça. Ensuite, pour la question des utilisations pour la défense et l'armement, certains de mes collègues un petit peu pacifistes disent qu'il faut absolument interdire les armes à base d'IA, ça peut être dangereux, etc. Et en fait, ce dont on s'aperçoit, c'est qu'aujourd'hui, l'IA commence à être utilisée massivement",
      "speaker": "SPEAKER1"
    },
    {
      "start": 5438.25,
      "end": 5438.44,
      "text": "– Sous-titrage ST' 501",
      "speaker": "SPEAKER0"
    },
    {
      "start": 5460.91,
      "end": 5490.91,
      "text": "défense de la démocratie en Ukraine. Les Ukrainiens se défendent grâce à des drones qui sont pilotés par des gens par radio et par vidéo. Mais la contre-mesure que les Russes emploient contre ça, c'est de brouiller les communications vidéo, audio. Donc, on ne peut plus piloter les drones. Il faut que les drones soient autonomes pour aller faire sauter un tank ou un canon. Et donc, ils travaillent massivement sur ce genre de technologies. Ça utilise des technologies d'intelligence artificielle, de reconnaissance des formes,",
      "speaker": "SPEAKER1"
    },
    {
      "start": 5462.84,
      "end": 5463.24,
      "text": "Hmm.",
      "speaker": "SPEAKER0"
    },
    {
      "start": 5490.91,
      "end": 5520.91,
      "text": "des réseaux convolutifs, très probablement. Je ne suis pas dans les secrets. Et beaucoup de gens, en fait, un de mes collègues, Antoine Borde, a quitté son poste à faire pour rejoindre une entreprise franco-allemande qui s'appelle Helsing et qui travaille sur l'intelligence artificielle pour la défense et qui est un petit peu critique pour la défense européenne. En fait, il y a des entreprises similaires aux Etats-Unis. Donc, ce n'est pas forcément mauvais d'utiliser ce genre de",
      "speaker": "SPEAKER1"
    },
    {
      "start": 5509.28,
      "end": 5509.6,
      "text": "OK.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 5520.38,
      "end": 5520.72,
      "text": "ce jour-là.",
      "speaker": "SPEAKER0"
    },
    {
      "start": 5520.91,
      "end": 5580.91,
      "text": "pour la défense, ça peut aider à la défense de la démocratie et du monde libre contre les régimes autoritaires, d'une certaine manière. Mais ce qui est important, c'est que dans la mesure où, dans le futur, on va tous interagir avec le monde numérique à travers des agents intelligents, on ne peut pas se permettre, justement pour la défense de la démocratie, que ces agents intelligents soient produits par deux ou trois entreprises sur la côte ouest des États-Unis. Il va falloir avoir accès à une très large diversité d'agents intelligents. Et la diversité serait pour le fait que ces systèmes puissent parler une grande quantité de langues différentes, y compris dialectes, et accès ou comprennent la culture locale, les systèmes de valeurs, les biais politiques, etc. Il est impossible de construire un système d'IA qui n'est pas biaisé. De même qu'il est impossible",
      "speaker": "SPEAKER1"
    },
    {
      "start": 5579.36,
      "end": 5579.78,
      "text": "Hmm.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 5580.91,
      "end": 5631.18,
      "text": "de faire un journal ou un magazine qui n'est pas biaisé au niveau politique ou opinion philosophique, etc., ou système de valeur. Et la solution qu'on connaît à ça, c'est On ne va pas prendre ces nouvelles avec un seul journal ou un seul magazine. On va avoir une diversité de journaux et de magazines qui nous va permettre de choisir un petit peu les biais pesés, etc. D'avoir une diversité de sources d'informations. Il va falloir que la même chose arrive pour les systèmes d'IA, c'est-à-dire que les assistants virtuels devront être de sources très diverses. Et ça, ça ne peut exister que si on a des plateformes open source. Parce qu'on ne peut pas spécialiser, enseigner un système propriétaire de parler Wolof ou une des 500 langues.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 5627.91,
      "end": 5632.16,
      "text": "Oui. On l'a vu aujourd'hui. En Inde où...",
      "speaker": "SPEAKER2"
    },
    {
      "start": 5632.1,
      "end": 5716.81,
      "text": "en Inde ou ailleurs, il faut que ces systèmes soient open source pour que les gens dans les communautés locales, les gouvernements, les associations, les petites boîtes puissent affiner ces systèmes et offrir une très grande diversité de systèmes spécialisés. Meta a été résolument pris la décision d'open sourcer ces modèles d'IA, en partie pour cette raison. Je pense que pour moi, c'est la raison principale, mais aussi pour la raison que ça fait progresser le domaine plus rapidement ça rend des systèmes plus sécurisés d'une part, on trouve les bugs plus facilement s'il y a plus de gens qui regardent. Ça essaime une industrie complète qui n'existerait pas sans les systèmes open source. Il y a 600 startups d'IA aujourd'hui à Paris, elles utilisent presque toutes des modèles open source. Donc c'est vraiment important. Et donc le danger, c'est qu'à cause de lobbying, réglementation peut-être de dynamique du marché qu'en fait le marché soit capturé par deux ou trois entreprises et qu'on n'ait pas accès à une grande diversité d'assistants d'IA je pense que c'est ça le plus gros danger à court terme puis il y a des dangers plus petits mais qui sont plus anciens est-ce qu'on peut utiliser l'IA pour disséminer la désinformation des fake news etc mais ça en fait c'est pas des nouveaux problèmes on sait déjà un petit peu les contrôler Facebook et Instagram",
      "speaker": "SPEAKER1"
    },
    {
      "start": 5714.21,
      "end": 5731.05,
      "text": "Allez. J'imagine que vous êtes au cœur de ces choses-là. En effet, c'est intéressant de voir ça aussi avec la masse de données. Et ça utilise l'IA massivement. J'imagine, parce qu'on a vu en effet tous ces reportages sur notamment la...",
      "speaker": "SPEAKER2"
    },
    {
      "start": 5719.34,
      "end": 5724.72,
      "text": "Oui. C'est qui?",
      "speaker": "SPEAKER1"
    },
    {
      "start": 5731.39,
      "end": 5737.23,
      "text": "En fait, le contrôle des choses qui sont postées, les données sont tellement énormes que...",
      "speaker": "SPEAKER2"
    },
    {
      "start": 5735.35,
      "end": 5797.23,
      "text": "Oui. En fait, il y a un chiffre qui est vraiment important. Fin 2017, on a fait une statistique, on a essayé de voir, de détecter tous les contenus de discours de haine sur Facebook. Et on a des systèmes automatiques d'intelligence artificielle, mais assez primitifs, fin 2017, qui suppriment automatiquement le discours de haine avant que qui que ce soit ne les voit. Et la proportion de discours de haine supprimés automatiquement 23% si je me rappelle bien, dans ces derniers trimestres 2017. Ça veut dire que les autres, le reste, les 77%, étaient en fait publiés sur Facebook, et puis ensuite signalés par des utilisateurs, et ensuite examinés par des modérateurs humains qui décidaient de les supprimer, en étant discours de haine. Ces discours de haine sont illégaux en Europe, certains. On ne peut pas faire la propagande néo-nazie, etc. Ce n'est pas illégaux,",
      "speaker": "SPEAKER1"
    },
    {
      "start": 5768.95,
      "end": 5769.36,
      "text": "Hum.",
      "speaker": "SPEAKER0"
    },
    {
      "start": 5794.55,
      "end": 5794.97,
      "text": "Oui.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 5794.97,
      "end": 5794.99,
      "text": "Merci.",
      "speaker": "SPEAKER0"
    },
    {
      "start": 5797.23,
      "end": 5827.23,
      "text": "Etats-Unis, mais on applique un peu les mêmes règles partout. Et ce pourcentage fin 2022, donc 5 ans plus tard, est passé à 96%. Et la différence, c'est les transformeurs avec apprentissage auto-supervisé. C'est-à-dire exactement la même technologie qui est utilisée dans les LLM a été en fait améliorée ce genre de score d'une manière absolument incroyable depuis 2017. Les transformeurs, ça date de 2017-2018.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 5806.95,
      "end": 5807.38,
      "text": "Incroyable.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 5827.23,
      "end": 5844.4,
      "text": "Il y a le processus d'autosuperviser généralisé à peu près à la même époque. Et en fait, ça a permis d'entraîner comment, par exemple, détecter que des gens s'échangent des arguments pour s'entretuer dans une région où il y a une guerre civile, qui parle un dialecte local.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 5842.76,
      "end": 5843.07,
      "text": "– OK.",
      "speaker": "SPEAKER0"
    },
    {
      "start": 5845.78,
      "end": 5886.94,
      "text": "qu'on ne parle pas, qu'aucun des modérateurs humains ne parle, parce qu'ils sont dans un pays où on ne peut pas s'installer, parce que le gouvernement ne nous aime pas. Donc c'est vraiment compliqué de faire ce genre de modération de contenu. Mais grâce aux transformeurs avec apprentissage autosupervisé, maintenant on peut entraîner ces systèmes-là à comprendre à peu près n'importe quelle langue dans le monde. Et donc, faire en sorte que les gens s'entretuent moins. On ne peut pas, malheureusement, avec les réseaux sociaux, empêcher que les gens s'entretuent. On voudrait bien, mais on n'a pas cette superpuissance. Et puis, protéger le processus électoral, les élections, la démocratie, etc. On avance d'une élection, il faut un peu essayer de calmer le jeu, que les gens ne s'aiment pas les isanis, etc.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 5888.11,
      "end": 5897.46,
      "text": "J'ai deux questions avant de te laisser partir. Une très rapide. Alors tu as un livre qui vient de sortir qui s'appelle « Quand la machine apprend ». Si tu avais, toi, un livre à me recommander, ce serait lequel?",
      "speaker": "SPEAKER2"
    },
    {
      "start": 5899.23,
      "end": 5902.97,
      "text": "Alors en fait, ce livre est écrit... Presque cinq ans.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 5902.97,
      "end": 5909.44,
      "text": "Ok, mais j'ai vu que tu le signais hier à Vivatech, alors je me suis dit, tu as peut-être un autre livre plus récent de ta part?",
      "speaker": "SPEAKER2"
    },
    {
      "start": 5905.81,
      "end": 5939.28,
      "text": "Voilà. Alors, ce livre est sorti en 2019, en français. Je l'ai écrit en français. J'ai eu de l'aide. Édité chez Odile Jacob. Et l'année dernière, il y a quelques mois, on a sorti une version poche. Donc, elle est relativement récente. Et effectivement, j'ai fait une séance de dédicaces à Vivatech. On a dû couper la queue. Enfin, il y a eu une heure et demie que.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 5914.2,
      "end": 5937.7,
      "text": "Ok. Donc elle est relativement récente. Très bien. De dédicaces. avec Mickey.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 5939.28,
      "end": 5944.92,
      "text": "Il y a eu beaucoup trop de gens qui étaient... Donc j'ai un fan club. Sous-titrage Société Radio-Canada",
      "speaker": "SPEAKER1"
    },
    {
      "start": 5944.92,
      "end": 5946.56,
      "text": "J'en suis pas président. Merci.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 5946.56,
      "end": 5946.98,
      "text": "Merci.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 5946.98,
      "end": 5947.81,
      "text": "Merci.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 5947.81,
      "end": 5948.28,
      "text": "Pour les enfants.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 5947.83,
      "end": 5950.54,
      "text": "On les invite, on les salue s'ils sont là encore.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 5949.75,
      "end": 5967.01,
      "text": "Je les salue et les remercie. J'ai entendu plein d'histoires très émouvantes de gens qui m'ont dit j'ai lu votre bouquin et ça a changé ma vie. J'ai complètement changé ma carrière. Je me suis mis à apprendre l'IA, j'ai créé une start-up, j'ai repris des études. Donc c'est très émouvant.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 5966.31,
      "end": 5968.8,
      "text": "C'est très émouvant. Il en aime. Je vous le dis. Merci.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 5968.8,
      "end": 5994.98,
      "text": "Merci. Il avait essayé de me recruter mais ça n'a pas marché. Ça fait longtemps. Et donc, c'est émouvant de savoir quel impact on peut avoir. C'est un petit peu compliqué pour moi à Vivatech parce que je ne pouvais pas faire 5 mètres sans que les gens me demandent de faire des selfies avec eux. Mais c'est pareil, je trouve ça charmant. C'est sympa. Voilà.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 5994.98,
      "end": 5995.06,
      "text": "Merci.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 5995.06,
      "end": 5996.46,
      "text": "Sous-titrage Société Radio-Canada Sous-titrage Société Radio-Canada",
      "speaker": "SPEAKER1"
    },
    {
      "start": 5996.46,
      "end": 6007.65,
      "text": "J'ai perdu le fil. Un livre que tu aurais à me recommander? Ça peut être... Peu importe, je sais que tu es dans plein d'autres choses. Tu fais de la musique, j'ai vu que tu faisais du modélisme. Tu as un site de modélisme avec... On voit ton fils Kevin, c'est ça?",
      "speaker": "SPEAKER2"
    },
    {
      "start": 6007.13,
      "end": 6009.42,
      "text": "Bon, ça c'est un peu ancien.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 6008.56,
      "end": 6025.79,
      "text": "Oui, c'est à mourir de rire. Mais je me demande comment tu as le temps de faire tout ça, toutes ces passions dont tu me parlais. Tu es arrivé, tu me parles de mes caméras, tu me dis, je ne sais pas comment tu fais tout ça. J'aurais pu rentrer là-dedans, mais on n'a plus le temps. Mais donc, un livre qui t'a marqué, si tu avais l'opportunité d'offrir un livre à tout le monde, ce serait lequel?",
      "speaker": "SPEAKER2"
    },
    {
      "start": 6026.65,
      "end": 6056.65,
      "text": "Ça peut peut-être te surprendre, mais c'est un petit livre écrit par Richard Feynman qui s'appelle QED. Donc QED en anglais. Je ne sais pas s'il y a une traduction française de ce livre, je pense qu'il doit y en avoir une. Donc QED, ça veut dire Quantum Electrodynamics, donc électrodynamique quantique. Et Feynman a ce don d'expliquer en termes très simples des concepts extrêmement compliqués de physique quantique, etc. Et d'expliquer en fait des phénomènes qu'on observe tous les jours,",
      "speaker": "SPEAKER1"
    },
    {
      "start": 6031.58,
      "end": 6035.34,
      "text": "Ok. Ok.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 6056.65,
      "end": 6066.76,
      "text": "Donc pourquoi, par exemple, quand on met un film d'huile sur de l'eau ou sur une surface plate, on voit des iridescences colorées?",
      "speaker": "SPEAKER1"
    },
    {
      "start": 6067.49,
      "end": 6072.8,
      "text": "Tu es en train de m'ouvrir le truc de la physique quantique et de l'IA, mais je ne vais pas aller là-dedans parce que je vais me faire gronder après par ton...",
      "speaker": "SPEAKER2"
    },
    {
      "start": 6073.02,
      "end": 6080.65,
      "text": "Ok, je vais dire deux mots. Je ne crois pas à l'application du calcul quantique à l'IA et en fait, je suis très sceptique sur le calcul quantique.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 6074.69,
      "end": 6090.07,
      "text": "Je ne crois pas. Très bien. On en fera un débat avec nos amis de Pascal. Voilà, exactement. Dans un prochain épisode.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 6086.25,
      "end": 6088.94,
      "text": "Voilà, exactement.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 6090.54,
      "end": 6107.38,
      "text": "Euh... C'est intéressant cette question que je pose à chaque fois quand même à mes invités, mais je pense que chez toi, elle pourrait avoir une autre portée. Parce que, en effet, tu as appris plein de choses depuis. Si tu avais l'occasion, ton doctorat, tu l'as fait où? Pierre et Marie Curie?",
      "speaker": "SPEAKER2"
    },
    {
      "start": 6107.38,
      "end": 6110.94,
      "text": "Ça s'appelait Pierre-Méracurie à l'époque, maintenant ça fait partie de Sorbonne.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 6111.19,
      "end": 6124,
      "text": "Sorbonne, si tu avais l'occasion, je ne sais pas moi, de croiser Yann Lequin au moment de l'obtention de ton doctorat et de te dire quelque chose, tu pouvais te glisser un mot derrière l'oreille, qu'est-ce que tu te dirais à ce moment-là?",
      "speaker": "SPEAKER2"
    },
    {
      "start": 6124,
      "end": 6163.57,
      "text": "Je me dirais, si tu vas aux Etats-Unis, tu vas apprendre que tu ne devrais pas avoir de complexe, de complexe d'infériorité. parce que t'es pas allé dans une des deux ou trois écoles les plus... Princeton, Stanford... Ou même en France, Normale Sup, Polytechnique, sans parler, j'ai fait une école qui s'appelle Essier, qui est une bonne école, mais qui n'est pas dans les super tops, dans l'espèce de hiérarchie qu'elle en croit ou pas, mais dans laquelle j'ai appris plein de choses, en fait, donc qui a été très bien pour moi.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 6138.84,
      "end": 6144.66,
      "text": "Princeton, Stanford... Ils ne le sentent pas le jour.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 6152.99,
      "end": 6153.26,
      "text": "Hmm.",
      "speaker": "SPEAKER0"
    },
    {
      "start": 6163.57,
      "end": 6223.57,
      "text": "Alors, je suis dans leur conseil scientifique maintenant. Je suis président du conseil scientifique de l'ICI. C'est une bonne école. Mais, donc, je dirais, ce n'est pas complexe. Et j'ai appris ça quelques années plus tard à Bell Labs, où je me suis aperçu, en fait, qu'aux États-Unis, on enseignait aux jeunes et aux étudiants de ne pas avoir de complètes infériorités, de ne pas hésiter à poser des questions stupides, de ne pas hésiter à se lancer dans des domaines dans lesquels on n'a pas de diplôme, pas nécessairement spécialiste, etc. Donc je me suis un petit peu essayé la physique par exemple dans les premières années où j'étais à Bell Labs parce qu'une grande partie du labo dans lequel j'étais était constituée de physiciens. Donc j'ai écrit deux ou trois articles publiés dans des journaux de physique sur la physique un petit peu fondamentale. Et en fait, il faut être ambitieux, avoir l'ambition, c'est-à-dire se fixer un but à très long terme qui pour moi a toujours été découvrir le mystère de l'intelligence et en tant qu'ingénieur aussi construire des machines intelligentes",
      "speaker": "SPEAKER1"
    },
    {
      "start": 6207.13,
      "end": 6207.48,
      "text": "physique.",
      "speaker": "SPEAKER0"
    },
    {
      "start": 6223.57,
      "end": 6283.57,
      "text": "je pense que c'est la seule manière de valider si des idées abstraites fonctionnent. Décrypter un peu comment l'intelligence a émergé chez les humains, chez les animaux, en le reproduisant dans les machines. Et ensuite, essayer de voir quel chemin je peux prendre, quel pas je peux faire en avant vers ce but à long terme. Et évidemment, remplir ce but à long terme peut prendre 10, 20, 30, 40 ans, mais on peut faire des progrès qui, à la fin, auront en fait un impact à un relativement court terme. Donc, par exemple, comment construire des machines intelligentes? Pour construire des machines intelligentes, on peut soit les concevoir soi-même, soit faire en sorte que ces machines se conçoivent par apprentissage. Ce qui est un peu l'idée. Donc, très rapidement, je me suis dit, je ne suis pas assez intelligent pour construire des machines intelligentes. Par contre, peut-être si elles peuvent apprendre, ce qui est le cas de tous les êtres vivants, peut-être elles peuvent, par auto-organisation, en fait, devenir intelligentes. Donc, l'apprentissage des machines, c'est vraiment le truc qu'il faut,",
      "speaker": "SPEAKER1"
    },
    {
      "start": 6279.67,
      "end": 6280.1,
      "text": "– En fait,",
      "speaker": "SPEAKER0"
    },
    {
      "start": 6283.57,
      "end": 6343.57,
      "text": "essayer d'attaquer. Donc j'ai regardé la littérature qui s'arrêtait dans les années 60 parce qu'il y a eu des échecs sur les limitations et je me suis dit mais quand même ça a l'air intéressant comme conseil, il faut essayer de pousser. Donc comment entraîner, comment résoudre le problème qui n'avait pas été résolu à l'époque, d'entraîner des réseaux de neurones à plusieurs couches. J'ai trouvé une idée qui était très similaire à ce qu'on appelle maintenant la rétropropagation de gradient. Je me suis aperçu qu'il y avait deux ou trois autres personnes dans le monde qui s'intéressent aux mêmes choses dont Geoffrey Hinton et quelques autres. Et puis, bon, quand j'ai fini mon doctorat là-dessus, je suis allé travailler avec lui pendant un an. Et puis, j'ai développé les réseaux convolutifs qui sont des architectures de réseaux de neurones qui sont un peu inspirées de la biologie, de l'architecture du cortex visuel. Parce que je me suis dit, la première chose qu'on peut faire avec ces trucs-là, c'est essayer de faire de la reconnaissance d'images, de la reconnaissance des formes. Et à l'époque, les seules images pour lesquelles on avait des bases données pour lesquelles on peut utiliser l'apprentissage, c'était des chiffres ou des lettres scannées.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 6333.44,
      "end": 6333.79,
      "text": "Hum.",
      "speaker": "SPEAKER0"
    },
    {
      "start": 6343.57,
      "end": 6345.23,
      "text": "D'accord.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 6344.09,
      "end": 6351.63,
      "text": "D'accord. En informatique, tu veux dire qu'on avait été numérisés à ce moment-là. Il n'y avait pas de photos, il n'y avait pas de vidéos. Il n'y avait pas de caméras.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 6347.07,
      "end": 6380.33,
      "text": "Il avait été numérisé avec... Il n'y avait pas de caméra USB. Il y avait... Capturer des images dans les ordinateurs, c'était vraiment compliqué. Il fallait acheter des gros trucs très chers. Mais bon, il y avait des projets un peu comme ça parce que ça intéressait la poste, d'automatiser le tri... La reconnaissance des codes postaux. Voilà, des codes postaux. Et puis, ça intéressait les banques de reconnaître les chiffres sur les chèques, etc. Donc finalement, il y avait un petit peu de données. On s'est attaché à essayer de voir la performance de ces systèmes.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 6355.98,
      "end": 6369.36,
      "text": "mais La reconnaissance du code posto. Voilà.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 6380.33,
      "end": 6410.33,
      "text": "mesuré par exemple ces systèmes pour ce genre de tâches de reconnaissance de chiffres. Et ça a marché extrêmement bien. Et donc c'était un petit pas. Et puis là, immédiatement, AT&T, l'entreprise qui chapeautait Bell Labs, on a créé un projet pour commercialiser ça. Donc faire un système de lecture de chèques, de lecture de formulaire d'inscription. Il n'y avait pas l'Internet. On parle de 89, 90. Et donc il y a eu des applications à court terme qui ont eu un impact. Mais le but n'était pas de faire la reconnaissance de caractère.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 6400.65,
      "end": 6400.8,
      "text": "Non.",
      "speaker": "SPEAKER0"
    },
    {
      "start": 6410.33,
      "end": 6465.57,
      "text": "sur une machine intelligente. Et puis, bon, on a développé ça. Puis finalement, l'intérêt de la communauté pour ça a un petit peu diminué dans les années 90. J'ai fait autre chose entre 96 et 2002. Je me suis remis à peu près en 2002. Et puis là, avec Jeff Hinton et Joshua Bengio, on s'est dit, il faut absolument raviver l'intérêt de la communauté pour ces méthodes parce qu'on sait qu'elles marchent. Et les raisons pour lesquelles la communauté pense qu'elles ne marchent pas, en fait, sont fausses. Donc, on s'est attaché à l'idée de montrer qu'elles marchaient. et puis de trouver de nouvelles méthodes. Et c'est ça qui a causé la réapparition des réseaux neurones et du deep learning. Et en partie grâce à la disponibilité d'ordinateurs beaucoup plus puissants qu'on avait à l'époque, et puis de bases de données, d'images, de textes, grâce à l'Internet, qui n'existaient pas dans les années 90, bien sûr.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 6458.53,
      "end": 6467.19,
      "text": "images hallucinantes. Grâce à l'internet. et grâce aux réseaux sociaux ensuite.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 6467.19,
      "end": 6468.29,
      "text": "Et grâce aux réseaux sociaux ensuite.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 6469.16,
      "end": 6516.85,
      "text": "Je crois qu'il est temps que je te laisse filer. Merci Yann, tu sais, c'est pour ce genre d'échange que je fais ce podcast. parce que j'ai envie de comprendre où est-ce qu'on est, où est-ce qu'on en est. sans jugement sur est-ce que c'est bien, est-ce que c'est pas bien. Tu vois, là aujourd'hui, j'ai quand même vraiment l'impression de comprendre plus de choses, beaucoup plus de choses. Donc je te remercie, j'espère que comme Lex Friedman, j'aurai l'occasion d'en avoir plus de Yann Lequin, peut-être une ou deux fois plus tard. On viendra à New York s'il le faut. En tout cas, je serai ravi. Et puis... Et puis écoute, je suis impatient de voir la suite et puis de pouvoir tester un peu toutes ces applications que je n'ai pas encore testées chez Meta.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 6517.29,
      "end": 6527.7,
      "text": "D'accord. Et puis, peut-être si on fait effectivement des progrès tels que je les ai décrits dans l'année qui vient, on pourra avoir une autre session. J'espère que ça va être utile à tes auditeurs.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 6527.7,
      "end": 6551.53,
      "text": "Je suis sûr que ça sera utile. Je pense que ça créera des vocations aussi chez des devs et chez des personnes. Et puis, ça va permettre de comprendre, je pense, beaucoup de choses. Parce que beaucoup de personnes ne parlent pas l'anglais. Je pense que l'audio est un moment important dans ces compréhensions. Tu vois, d'avoir la durée qu'on a eue, c'est vraiment quelque chose de très important. Merci beaucoup. On te suit sur LinkedIn. Où est-ce que tu publies un peu?",
      "speaker": "SPEAKER2"
    },
    {
      "start": 6551.53,
      "end": 6581.53,
      "text": "Je publie un peu partout, sur LinkedIn, sur... Alors pas beaucoup sur Insta, plutôt sur Threads. Ok, tout à fait. Mais j'aime bien le texte, Insta c'est un peu trop centré sur la photo. Donc sur Threads, enfin Instagram Threads, sur X, sur LinkedIn, et sur Facebook. Et sur Facebook, en fait, les discussions sont plus intellectuelles que sur les autres plateformes. X c'est plus, enfin X Twitter,",
      "speaker": "SPEAKER1"
    },
    {
      "start": 6552.58,
      "end": 6558.97,
      "text": "sur sur l'Antille. Ok, j'ai fait.",
      "speaker": "SPEAKER2"
    },
    {
      "start": 6558.97,
      "end": 6559.21,
      "text": "Ça va?",
      "speaker": "SPEAKER0"
    },
    {
      "start": 6581.53,
      "end": 6608.23,
      "text": "c'est plus news rapide et puis des gens qui s'entretuent. Mais Facebook, c'est plus intellectuel. Et puis LinkedIn, c'est un peu plus professionnel, mais j'ai beaucoup de... Sur X et LinkedIn, je crois que j'ai 700 000 followers sur chacune des deux. Et puis Thread, c'est une nouvelle plateforme, c'est un peu intéressant.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 6584.4,
      "end": 6584.76,
      "text": "Ouais ouais ouais",
      "speaker": "SPEAKER0"
    },
    {
      "start": 6608.43,
      "end": 6638.43,
      "text": "intéressant j'y suis pas assez en effet mais tu veux peut-être pas quand même réussir à me faire revenir sur Facebook on sait pas on sait pas peut-être si tu me dis que c'est plus intellectuel c'est plus intellectuel y'a pas de doute mais c'est possible en fait j'y suis toujours mais j'y vais assez peu parce que j'ai peu d'usage merci Yann le cas on te suit partout merci aussi à Violaine Grécier qui nous a aidé à faire ça merci Yann Sophie de nous avoir permis de faire tout ça aussi et puis à Julien Codorniou qu'on embrasse qui est tellement important",
      "speaker": "SPEAKER2"
    },
    {
      "start": 6613.6,
      "end": 6616.97,
      "text": "Peut-être. Si tu veux, il n'y a pas de doute.",
      "speaker": "SPEAKER1"
    },
    {
      "start": 6638.43,
      "end": 6668.43,
      "text": "Pour Génération Nature Self qui nous aide à avoir beaucoup d'invités qui écoutent et qui font ces retours qui m'aident à progresser. On t'embrasse Julien. Merci Yann. A plus. Si vous êtes nouveau sur Génération Nature Self, je vous le demande à chaque fois, mais il y en a plein qui ne l'ont pas fait, appuyez sur follow, appuyez sur like et partagez cet épisode à deux personnes qui seront intéressées. J'espère qu'il est assez compréhensible pour tout le monde. En tout cas, il a été pour moi et comme j'ai bossé, contrairement à ce que j'ai dit avec beaucoup de personnes, je crois que je ne bosse qu'avec",
      "speaker": "SPEAKER2"
    },
    {
      "start": 6649.39,
      "end": 6649.93,
      "text": "Merci.",
      "speaker": "SPEAKER0"
    },
    {
      "start": 6668.43,
      "end": 6675.76,
      "text": "des gens plus intelligents que moi, je me pose des questions d'ailleurs, eh bien, vous devriez le comprendre aussi. A plus, salut, merci beaucoup.",
      "speaker": "SPEAKER2"
    }
  ]
}